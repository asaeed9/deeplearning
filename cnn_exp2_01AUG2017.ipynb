{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/asaeed9/work/deeplearning\n"
     ]
    }
   ],
   "source": [
    "%cd ~/work/deeplearning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from theano.sandbox import cuda\n",
    "cuda.use('gpu1')\n",
    "%matplotlib inline\n",
    "from __future__ import print_function, division\n",
    "#path=\"../data/2cat/sample\"\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "from IPython.display import FileLink\n",
    "from keras.preprocessing import image, sequence\n",
    "import os, sys, cv2\n",
    "from shutil import copyfile, move\n",
    "from random import shuffle\n",
    "\n",
    "####\n",
    "from keras.layers.convolutional import *\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.optimizers import SGD, RMSprop, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/asaeed9/work/data/2cat/train\n"
     ]
    }
   ],
   "source": [
    "data_dir=\"/home/asaeed9/work/data/2cat\"\n",
    "path=\"/home/asaeed9/work/data/2cat/sample/\"\n",
    "results_path = \"/home/asaeed9/work/data/2cat/sample/results\"\n",
    "test_path = path + '/test/' #We use all the test data\n",
    "%cd ../data/2cat/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#grey images - minimal image augmentation\n",
    "\n",
    "    \n",
    "    #clean previous data\n",
    "#     os.chdir(path + 'test')\n",
    "    #adjust_prev_data()\n",
    "    \n",
    "    #build validation set\n",
    "#     g = glob('*.jpg')\n",
    "#     shuf = np.random.permutation(g)\n",
    "#     for i in range(validation): os.rename(shuf[i], '../valid/' + shuf[i]) \n",
    "    \n",
    "#     g = glob('*.jpg')\n",
    "#     shuf = np.random.permutation(g)\n",
    "#     for i in range(train): os.rename(shuf[i], '../sample/train/' + shuf[i]) \n",
    "#     %mv ../sample/train/cat*.jpg ../sample/train/cats/\n",
    "#     %mv ../sample/train/dog*.jpg ../sample/train/dogs/\n",
    "    \n",
    "#     %cd ../valid\n",
    "\n",
    "#     g = glob('*.jpg')\n",
    "#     shuf = np.random.permutation(g)\n",
    "#     for i in range(validation): os.rename(shuf[i], '../sample/valid/' + shuf[i]) \n",
    "#     %mv ../sample/valid/cat*.jpg ../sample/valid/cats/\n",
    "#     %mv ../sample/valid/dog*.jpg ../sample/valid/dogs/\n",
    "#     %cd $data_dir/train  \n",
    "\n",
    "\n",
    "def adjust_prev_data_sample():\n",
    "    %mv $path/train/cats/* $data_dir/train/\n",
    "    %mv $path/train/dogs/* $data_dir/train/\n",
    "    %mv $path/valid/cats/* $data_dir/train/\n",
    "    %mv $path/valid/dogs/* $data_dir/train/\n",
    "\n",
    "#clean previous data\n",
    "def adjust_prev_data():\n",
    "    %mv $data_dir/valid/* $data_dir/train/    \n",
    "    adjust_prev_data_sample()\n",
    "\n",
    "#move training images\n",
    "def move_from_test(new_trainset, path, train, validation):\n",
    "    print(new_trainset[:10])\n",
    "      \n",
    "    \n",
    "#copy training images\n",
    "def copy_samples(train, validation):\n",
    "    #print(\"Copying new samples for training...\")\n",
    "    #clean previous data\n",
    "    adjust_prev_data()\n",
    "    \n",
    "    #build validation set\n",
    "    g = glob('*.jpg')\n",
    "    shuf = np.random.permutation(g)\n",
    "    for i in range(validation): os.rename(shuf[i], '../valid/' + shuf[i]) \n",
    "    \n",
    "    g = glob('*.jpg')\n",
    "    shuf = np.random.permutation(g)\n",
    "    for i in range(train): copyfile(shuf[i], '../sample/train/' + shuf[i]) \n",
    "    %mv ../sample/train/cat*.jpg ../sample/train/cats/\n",
    "    %mv ../sample/train/dog*.jpg ../sample/train/dogs/\n",
    "    \n",
    "    %cd ../valid\n",
    "\n",
    "    g = glob('*.jpg')\n",
    "    shuf = np.random.permutation(g)\n",
    "    for i in range(validation): copyfile(shuf[i], '../sample/valid/' + shuf[i]) \n",
    "    %mv ../sample/valid/cat*.jpg ../sample/valid/cats/\n",
    "    %mv ../sample/valid/dog*.jpg ../sample/valid/dogs/\n",
    "    %cd $data_dir/train\n",
    "\n",
    "def onehot(x): return np.array(OneHotEncoder().fit_transform(x.reshape(-1,1)).todense())\n",
    "\n",
    "def pred_batch(imgs, classes):\n",
    "    preds = model.predict(imgs)\n",
    "    idxs = np.argmax(preds, axis=1)\n",
    "\n",
    "    print('Shape: {}'.format(preds.shape))\n",
    "    print('First 5 classes: {}'.format(classes[:5]))\n",
    "    print('First 5 probabilities: {}\\n'.format(preds[:5]))\n",
    "    print('Predictions prob/class: ')\n",
    "    \n",
    "    for i in range(len(idxs)):\n",
    "        idx = idxs[i]\n",
    "        print ('  {:.4f}/{}'.format(preds[i, idx], classes[idx]))\n",
    "\n",
    "\n",
    "def generate_size_graph(fig_no, training_size, accuracy, loss, start_size, end_size):\n",
    "    plt.figure(fig_no,figsize=(7,5))\n",
    "    plt.plot(training_size,accuracy)\n",
    "    plt.plot(training_size,loss)\n",
    "    plt.xlabel('Training Size')\n",
    "    plt.ylabel('Accuracy/Loss')\n",
    "    plt.title('Training Size vs Accuracy/Loss')\n",
    "    plt.grid(True)\n",
    "    plt.legend(['Accuracy','Loss'])\n",
    "    plt.style.use(['classic'])\n",
    "    plt.show()\n",
    "    plt.savefig(path + '/batch_graphs/' +  str(start_size) + '_' + str(end_size) + '.jpg') \n",
    "        \n",
    "def generate_graph(fig_no, epochs, train, val, label, train_title, val_title, train_size):\n",
    "    plt.figure(fig_no,figsize=(7,5))\n",
    "    plt.plot(epochs,train)\n",
    "    plt.plot(epochs,val)\n",
    "    plt.xlabel('num of Epochs')\n",
    "    plt.ylabel(label)\n",
    "    plt.title(train_title + ' vs ' + val_title + '( Samples:' + str(train_size) + ')')\n",
    "    plt.grid(True)\n",
    "    plt.legend(['train','val'])\n",
    "    plt.style.use(['classic'])\n",
    "    plt.show()\n",
    "    plt.savefig(results_path + '/batch_graphs/' +  label + '_' + str(train_size) + '.jpg') \n",
    "    \n",
    "def get_train_model(tr_batches, val_batches, epoch):\n",
    "    model = Sequential([\n",
    "            BatchNormalization(axis=1, input_shape=(3, 256,256)),\n",
    "            Convolution2D(32,3,3, activation='relu'),\n",
    "            BatchNormalization(axis=1),\n",
    "            MaxPooling2D((3,3)),\n",
    "            Dropout(0.2),\n",
    "#             Convolution2D(64,3,3, activation='relu'),\n",
    "#             BatchNormalization(axis=1),\n",
    "            #MaxPooling2D((3,3)),\n",
    "            Convolution2D(64,3,3, activation='relu'),\n",
    "            BatchNormalization(axis=1),\n",
    "            MaxPooling2D((3,3)),\n",
    "            Dropout(0.2),\n",
    "            Convolution2D(32,3,3, activation='relu'),\n",
    "            BatchNormalization(axis=1),\n",
    "            MaxPooling2D((3,3)),\n",
    "            Dropout(0.2),\n",
    "            Flatten(),\n",
    "            Dense(1024, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.2),\n",
    "            Dense(2, activation='softmax')\n",
    "        ])\n",
    "\n",
    "    model.compile(Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit_generator(tr_batches, tr_batches.nb_sample, nb_epoch=2, validation_data=val_batches, \n",
    "                     nb_val_samples=val_batches.nb_sample)\n",
    "    model.optimizer.lr = 0.001\n",
    "    model.fit_generator(tr_batches, tr_batches.nb_sample, nb_epoch=epoch - 2, validation_data=val_batches, \n",
    "                     nb_val_samples=val_batches.nb_sample)\n",
    "    return model \n",
    "\n",
    "def train_model(model, tr_batches, val_batches, epoch):\n",
    "    if not model:\n",
    "        model = Sequential([\n",
    "                BatchNormalization(axis=1, input_shape=(3, 256,256)),\n",
    "                Convolution2D(32,3,3, activation='relu'),\n",
    "                BatchNormalization(axis=1),\n",
    "                MaxPooling2D((3,3)),\n",
    "                Dropout(0.2),\n",
    "    #             Convolution2D(64,3,3, activation='relu'),\n",
    "    #             BatchNormalization(axis=1),\n",
    "                #MaxPooling2D((3,3)),\n",
    "                Convolution2D(64,3,3, activation='relu'),\n",
    "                BatchNormalization(axis=1),\n",
    "                MaxPooling2D((3,3)),\n",
    "                Dropout(0.2),\n",
    "                Convolution2D(32,3,3, activation='relu'),\n",
    "                BatchNormalization(axis=1),\n",
    "                MaxPooling2D((3,3)),\n",
    "                Dropout(0.2),\n",
    "                Flatten(),\n",
    "                Dense(1024, activation='relu'),\n",
    "                BatchNormalization(),\n",
    "                Dropout(0.2),\n",
    "                Dense(2, activation='softmax')\n",
    "            ])\n",
    "        model.compile(Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "       \n",
    "    model.fit_generator(tr_batches, tr_batches.nb_sample, nb_epoch=2, validation_data=val_batches, \n",
    "                     nb_val_samples=val_batches.nb_sample)\n",
    "    model.optimizer.lr = 0.001\n",
    "    model.fit_generator(tr_batches, tr_batches.nb_sample, nb_epoch=epoch - 2, validation_data=val_batches, \n",
    "                     nb_val_samples=val_batches.nb_sample)\n",
    "        \n",
    "    return model \n",
    "    \n",
    "    \n",
    "def get_test_model():\n",
    "    model = Sequential([\n",
    "                BatchNormalization(axis=1, input_shape=(3,256,256)),\n",
    "                Convolution2D(32,3,3, activation='relu'),\n",
    "                BatchNormalization(axis=1),\n",
    "                MaxPooling2D((3,3)),\n",
    "    #             Convolution2D(64,3,3, activation='relu'),\n",
    "    #             BatchNormalization(axis=1),\n",
    "                #MaxPooling2D((3,3)),\n",
    "                Convolution2D(64,3,3, activation='relu'),\n",
    "                BatchNormalization(axis=1),\n",
    "                MaxPooling2D((3,3)),\n",
    "                Convolution2D(32,3,3, activation='relu'),\n",
    "                BatchNormalization(axis=1),\n",
    "                MaxPooling2D((3,3)),\n",
    "                Flatten(),\n",
    "                Dense(1024, activation='relu'),\n",
    "                BatchNormalization(),\n",
    "                Dense(2, activation='softmax')\n",
    "            ])\n",
    "\n",
    "    model.compile(Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])  \n",
    "    \n",
    "    return model\n",
    "\n",
    "def fit(samples_copied, old_model, path, results_path, nepoch, batch_size, train_size, valid_size):\n",
    "    gen_t = image.ImageDataGenerator(rotation_range=15, height_shift_range=0.05, \n",
    "                shear_range=0.1, channel_shift_range=20, width_shift_range=0.1)\n",
    "\n",
    "    #for train in training_range:\n",
    "    model = None\n",
    "    if not samples_copied:\n",
    "        copy_samples(train_size, valid_size)\n",
    "\n",
    "    tr_batches = gen_t.flow_from_directory(path + 'train', batch_size=batch_size)\n",
    "    val_batches = gen_t.flow_from_directory(path + 'valid', class_mode='categorical', shuffle=True, batch_size=batch_size * 2)\n",
    "    \n",
    "    if old_model:\n",
    "        model = train_model(old_model, tr_batches, val_batches, nepoch)\n",
    "    else:\n",
    "        model = train_model(None, tr_batches, val_batches, nepoch)\n",
    "        \n",
    "    model.save_weights(results_path+ '/' + 'ft_' + str(train_size) + '.e' + str(nepoch))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def predict(path, model):\n",
    "    gen_test = image.ImageDataGenerator()\n",
    "    test_batches = gen_test.flow_from_directory(path+'test', class_mode=None, target_size=(256,256), shuffle=False, batch_size=1)\n",
    "    test_data = np.concatenate([test_batches.next() for i in range(test_batches.nb_sample)])\n",
    "    test_labels = onehot(test_batches.classes)\n",
    "    score = model.evaluate(test_data, test_labels)\n",
    "    \n",
    "    probs = model.predict(test_data)\n",
    "    \n",
    "    #loss_score.append(score[0])\n",
    "    #accuracy_score.append(score[1])\n",
    "    \n",
    "    print(\"\\nLoss:{}, Accuracy:{}\".format(score[0], score[1]))\n",
    "    #print(\"\\nProbabilities:{}\".format(probs))\n",
    "    return probs, test_batches\n",
    "\n",
    "def move_samples(retrain_set,dest_path, n, limit):\n",
    "    cats_copied = 0\n",
    "    dogs_copied = 0\n",
    "    retrain_list = list(retrain_set)\n",
    "    shuffle(retrain_list)\n",
    "    for fil in range(n):\n",
    "        fil = retrain_list.pop()\n",
    "        fil_cpy = fil[fil.find('/')+1:]\n",
    "\n",
    "        if 'cat' in fil_cpy and cats_copied <= limit:\n",
    "            os.rename(os.path.join(path + \"test/cats/\"+ fil_cpy), os.path.join(path + dest_path + \"/cats/\"+ fil_cpy))\n",
    "            cats_copied+=1\n",
    "        elif 'dog' in fil_cpy and dogs_copied <= limit:\n",
    "            os.rename(os.path.join(path + \"test/dogs/\"+ fil_cpy), os.path.join(path + dest_path + \"/dogs/\" + fil_cpy))\n",
    "            dogs_copied+=1\n",
    "            \n",
    "    #print(\"Limit:\", limit)        \n",
    "    #print(\"moved cats:\", cats_copied)\n",
    "    #print(\"moved dogs:\", dogs_copied)\n",
    "    #print(\"Retrain Length: \", len(retrain_list))\n",
    "    return retrain_list, cats_copied, dogs_copied\n",
    "\n",
    "\n",
    "def move_to_train(retrain_set, limit):\n",
    "    cats = 0\n",
    "    dogs = 0\n",
    "    valid_limit = int(math.floor(.2*(limit*2)))\n",
    "    train_limit = int(math.floor(.8*(limit*2)))\n",
    "    \n",
    "    #print(retrain_set[:10])\n",
    "    print(\"validation set: \", valid_limit)\n",
    "    retrain_list, cats_copied, dogs_copied = move_samples(retrain_set, \"valid\", valid_limit, limit)\n",
    "    valid_size = cats_copied + dogs_copied\n",
    "    print(\"Train set: \", train_limit)\n",
    "    retrain_list, cats_copied, dogs_copied = move_samples(retrain_list, \"train\", train_limit, limit)\n",
    "    train_size = cats_copied + dogs_copied\n",
    "    \n",
    "    return train_size, valid_size, train_size + valid_size\n",
    "\n",
    "def refil_test(nimages):\n",
    "    os.chdir(\"/home/asaeed9/work/data/2cat/train\")\n",
    "    g = glob('*.jpg')\n",
    "    shuf = np.random.permutation(g)\n",
    "    for i in range(nimages): os.rename(shuf[i], '../sample/test/' + shuf[i])\n",
    "    #os.chdir(\"../sample/test/\")\n",
    "    #move(\"cat*.jpg\")\n",
    "    %mv ../sample/test/cat*.jpg ../sample/test/cats/\n",
    "    %mv ../sample/test/dog*.jpg ../sample/test/dogs/     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/asaeed9/work/data/2cat/train\n",
      "/home/asaeed9/work/data/2cat/valid\n",
      "/home/asaeed9/work/data/2cat/train\n",
      "Found 100 images belonging to 2 classes.\n",
      "Found 20 images belonging to 2 classes.\n",
      "Epoch 1/2\n",
      "100/100 [==============================] - 1s - loss: 0.9318 - acc: 0.5900 - val_loss: 0.7443 - val_acc: 0.5500\n",
      "Epoch 2/2\n",
      "100/100 [==============================] - 1s - loss: 0.9035 - acc: 0.5700 - val_loss: 0.6656 - val_acc: 0.6000\n",
      "Epoch 1/13\n",
      "100/100 [==============================] - 1s - loss: 0.6460 - acc: 0.7000 - val_loss: 0.6784 - val_acc: 0.6500\n",
      "Epoch 2/13\n",
      "100/100 [==============================] - 1s - loss: 0.6857 - acc: 0.6200 - val_loss: 0.6952 - val_acc: 0.6000\n",
      "Epoch 3/13\n",
      "100/100 [==============================] - 1s - loss: 0.6260 - acc: 0.7100 - val_loss: 0.7035 - val_acc: 0.4500\n",
      "Epoch 4/13\n",
      "100/100 [==============================] - 1s - loss: 0.8512 - acc: 0.6700 - val_loss: 0.7018 - val_acc: 0.5500\n",
      "Epoch 5/13\n",
      "100/100 [==============================] - 1s - loss: 0.5858 - acc: 0.7200 - val_loss: 0.6822 - val_acc: 0.4500\n",
      "Epoch 6/13\n",
      "100/100 [==============================] - 1s - loss: 0.5850 - acc: 0.7800 - val_loss: 0.7404 - val_acc: 0.5500\n",
      "Epoch 7/13\n",
      "100/100 [==============================] - 1s - loss: 0.8586 - acc: 0.6300 - val_loss: 0.7150 - val_acc: 0.4500\n",
      "Epoch 8/13\n",
      "100/100 [==============================] - 1s - loss: 0.4948 - acc: 0.7400 - val_loss: 0.7826 - val_acc: 0.5000\n",
      "Epoch 9/13\n",
      "100/100 [==============================] - 1s - loss: 0.6078 - acc: 0.7400 - val_loss: 0.8357 - val_acc: 0.5000\n",
      "Epoch 10/13\n",
      "100/100 [==============================] - 1s - loss: 0.4405 - acc: 0.8100 - val_loss: 0.9191 - val_acc: 0.5000\n",
      "Epoch 11/13\n",
      "100/100 [==============================] - 1s - loss: 0.4817 - acc: 0.7800 - val_loss: 0.9594 - val_acc: 0.5000\n",
      "Epoch 12/13\n",
      "100/100 [==============================] - 1s - loss: 0.6309 - acc: 0.7900 - val_loss: 1.1191 - val_acc: 0.5000\n",
      "Epoch 13/13\n",
      "100/100 [==============================] - 1s - loss: 0.5245 - acc: 0.7700 - val_loss: 1.1409 - val_acc: 0.5000\n",
      "Found 3176 images belonging to 2 classes.\n",
      "3168/3176 [============================>.] - ETA: 0s\n",
      "Loss:1.14563505948, Accuracy:0.50661209068\n",
      "validation set:  4\n",
      "Train set:  19\n",
      "Found 19 images belonging to 2 classes.\n",
      "Found 4 images belonging to 2 classes.\n",
      "Epoch 1/2\n",
      "19/19 [==============================] - 0s - loss: 1.7506 - acc: 0.2632 - val_loss: 4.0771 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "19/19 [==============================] - 0s - loss: 1.1186 - acc: 0.4211 - val_loss: 2.3641 - val_acc: 0.2500\n",
      "Epoch 1/13\n",
      "19/19 [==============================] - 0s - loss: 0.9280 - acc: 0.5263 - val_loss: 2.1756 - val_acc: 0.2500\n",
      "Epoch 2/13\n",
      "19/19 [==============================] - 0s - loss: 0.5891 - acc: 0.6842 - val_loss: 2.3722 - val_acc: 0.2500\n",
      "Epoch 3/13\n",
      "19/19 [==============================] - 0s - loss: 0.8443 - acc: 0.6842 - val_loss: 1.9147 - val_acc: 0.2500\n",
      "Epoch 4/13\n",
      "19/19 [==============================] - 0s - loss: 0.7977 - acc: 0.6842 - val_loss: 1.6840 - val_acc: 0.2500\n",
      "Epoch 5/13\n",
      "19/19 [==============================] - 0s - loss: 0.6088 - acc: 0.7368 - val_loss: 1.4283 - val_acc: 0.2500\n",
      "Epoch 6/13\n",
      "19/19 [==============================] - 0s - loss: 0.4301 - acc: 0.8421 - val_loss: 1.5105 - val_acc: 0.2500\n",
      "Epoch 7/13\n",
      "19/19 [==============================] - 0s - loss: 0.7803 - acc: 0.5263 - val_loss: 1.1966 - val_acc: 0.2500\n",
      "Epoch 8/13\n",
      "19/19 [==============================] - 0s - loss: 0.4792 - acc: 0.7368 - val_loss: 0.9397 - val_acc: 0.5000\n",
      "Epoch 9/13\n",
      "19/19 [==============================] - 0s - loss: 0.4997 - acc: 0.7368 - val_loss: 0.8765 - val_acc: 0.5000\n",
      "Epoch 10/13\n",
      "19/19 [==============================] - 0s - loss: 0.7083 - acc: 0.6316 - val_loss: 1.1049 - val_acc: 0.5000\n",
      "Epoch 11/13\n",
      "19/19 [==============================] - 0s - loss: 0.4209 - acc: 0.7895 - val_loss: 0.7353 - val_acc: 0.5000\n",
      "Epoch 12/13\n",
      "19/19 [==============================] - 0s - loss: 0.7823 - acc: 0.6842 - val_loss: 0.7374 - val_acc: 0.5000\n",
      "Epoch 13/13\n",
      "19/19 [==============================] - 0s - loss: 0.2285 - acc: 0.8947 - val_loss: 0.7632 - val_acc: 0.5000\n",
      "Found 3176 images belonging to 2 classes.\n",
      "3168/3176 [============================>.] - ETA: 0s\n",
      "Loss:0.72411022446, Accuracy:0.498740554156\n",
      "validation set:  534\n",
      "Train set:  2137\n",
      "Found 2137 images belonging to 2 classes.\n",
      "Found 534 images belonging to 2 classes.\n",
      "Epoch 1/2\n",
      "2137/2137 [==============================] - 31s - loss: 1.0400 - acc: 0.5438 - val_loss: 1.0021 - val_acc: 0.5150\n",
      "Epoch 2/2\n",
      "2137/2137 [==============================] - 27s - loss: 0.9379 - acc: 0.5728 - val_loss: 1.3349 - val_acc: 0.5131\n",
      "Epoch 1/13\n",
      "2137/2137 [==============================] - 31s - loss: 0.8636 - acc: 0.5966 - val_loss: 1.4737 - val_acc: 0.4944\n",
      "Epoch 2/13\n",
      "2137/2137 [==============================] - 28s - loss: 0.8094 - acc: 0.6224 - val_loss: 1.2947 - val_acc: 0.5262\n",
      "Epoch 3/13\n",
      "2137/2137 [==============================] - 27s - loss: 0.7750 - acc: 0.6280 - val_loss: 1.1592 - val_acc: 0.5356\n",
      "Epoch 4/13\n",
      "2137/2137 [==============================] - 28s - loss: 0.7852 - acc: 0.6434 - val_loss: 1.0098 - val_acc: 0.6105\n",
      "Epoch 5/13\n",
      "2137/2137 [==============================] - 28s - loss: 0.7159 - acc: 0.6598 - val_loss: 0.9131 - val_acc: 0.5974\n",
      "Epoch 6/13\n",
      "2137/2137 [==============================] - 28s - loss: 0.7109 - acc: 0.6593 - val_loss: 1.0153 - val_acc: 0.6330\n",
      "Epoch 7/13\n",
      "2137/2137 [==============================] - 28s - loss: 0.7248 - acc: 0.6682 - val_loss: 1.0509 - val_acc: 0.5918\n",
      "Epoch 8/13\n",
      "2137/2137 [==============================] - 27s - loss: 0.6555 - acc: 0.6865 - val_loss: 1.0528 - val_acc: 0.6161\n",
      "Epoch 9/13\n",
      "2137/2137 [==============================] - 27s - loss: 0.6744 - acc: 0.6837 - val_loss: 1.0645 - val_acc: 0.6124\n",
      "Epoch 10/13\n",
      "2137/2137 [==============================] - 27s - loss: 0.6257 - acc: 0.6977 - val_loss: 1.0265 - val_acc: 0.5974\n",
      "Epoch 11/13\n",
      "2137/2137 [==============================] - 28s - loss: 0.6296 - acc: 0.7089 - val_loss: 1.0433 - val_acc: 0.6292\n",
      "Epoch 12/13\n",
      "2137/2137 [==============================] - 27s - loss: 0.6264 - acc: 0.6972 - val_loss: 1.1304 - val_acc: 0.5899\n",
      "Epoch 13/13\n",
      "2137/2137 [==============================] - 27s - loss: 0.6240 - acc: 0.6935 - val_loss: 0.9652 - val_acc: 0.6311\n",
      "Found 3176 images belonging to 2 classes.\n",
      "3168/3176 [============================>.] - ETA: 0s\n",
      "Loss:1.0326124227, Accuracy:0.607997481108\n",
      "validation set:  85\n",
      "Train set:  342\n",
      "Found 319 images belonging to 2 classes.\n",
      "Found 85 images belonging to 2 classes.\n",
      "Epoch 1/2\n",
      "319/319 [==============================] - 5s - loss: 1.0363 - acc: 0.5141 - val_loss: 1.8739 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "319/319 [==============================] - 3s - loss: 1.0545 - acc: 0.5643 - val_loss: 0.9735 - val_acc: 0.6353\n",
      "Epoch 1/13\n",
      "319/319 [==============================] - 5s - loss: 1.0021 - acc: 0.5517 - val_loss: 0.6745 - val_acc: 0.5294\n",
      "Epoch 2/13\n",
      "319/319 [==============================] - 3s - loss: 1.0113 - acc: 0.5517 - val_loss: 0.6406 - val_acc: 0.6588\n",
      "Epoch 3/13\n",
      "319/319 [==============================] - 4s - loss: 0.9602 - acc: 0.5799 - val_loss: 0.6151 - val_acc: 0.6235\n",
      "Epoch 4/13\n",
      "319/319 [==============================] - 3s - loss: 0.8574 - acc: 0.6364 - val_loss: 0.6650 - val_acc: 0.6000\n",
      "Epoch 5/13\n",
      "319/319 [==============================] - 4s - loss: 0.7819 - acc: 0.6426 - val_loss: 0.6716 - val_acc: 0.5412\n",
      "Epoch 6/13\n",
      "319/319 [==============================] - 3s - loss: 0.7729 - acc: 0.6332 - val_loss: 0.6860 - val_acc: 0.6118\n",
      "Epoch 7/13\n",
      "319/319 [==============================] - 4s - loss: 0.7562 - acc: 0.6364 - val_loss: 0.6930 - val_acc: 0.5882\n",
      "Epoch 8/13\n",
      "319/319 [==============================] - 3s - loss: 0.7635 - acc: 0.6771 - val_loss: 0.7155 - val_acc: 0.5176\n",
      "Epoch 9/13\n",
      "319/319 [==============================] - 4s - loss: 0.7296 - acc: 0.6771 - val_loss: 0.7445 - val_acc: 0.4471\n",
      "Epoch 10/13\n",
      "319/319 [==============================] - 3s - loss: 0.6661 - acc: 0.6803 - val_loss: 0.8023 - val_acc: 0.4353\n",
      "Epoch 11/13\n",
      "319/319 [==============================] - 4s - loss: 0.7571 - acc: 0.6928 - val_loss: 0.7547 - val_acc: 0.5176\n",
      "Epoch 12/13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319/319 [==============================] - 3s - loss: 0.6750 - acc: 0.7179 - val_loss: 0.7721 - val_acc: 0.4706\n",
      "Epoch 13/13\n",
      "319/319 [==============================] - 4s - loss: 0.6735 - acc: 0.6928 - val_loss: 0.8550 - val_acc: 0.4588\n",
      "Found 3176 images belonging to 2 classes.\n",
      "3168/3176 [============================>.] - ETA: 0s\n",
      "Loss:0.757014365881, Accuracy:0.549748110831\n",
      "validation set:  326\n",
      "Train set:  1305\n",
      "Found 1305 images belonging to 2 classes.\n",
      "Found 326 images belonging to 2 classes.\n",
      "Epoch 1/2\n",
      "1305/1305 [==============================] - 20s - loss: 1.0313 - acc: 0.5448 - val_loss: 0.7152 - val_acc: 0.5000\n",
      "Epoch 2/2\n",
      "1305/1305 [==============================] - 16s - loss: 0.8903 - acc: 0.6061 - val_loss: 1.2893 - val_acc: 0.4693\n",
      "Epoch 1/13\n",
      "1305/1305 [==============================] - 19s - loss: 0.8095 - acc: 0.6161 - val_loss: 2.2763 - val_acc: 0.4387\n",
      "Epoch 2/13\n",
      "1305/1305 [==============================] - 16s - loss: 0.7810 - acc: 0.6521 - val_loss: 2.3607 - val_acc: 0.4571\n",
      "Epoch 3/13\n",
      "1305/1305 [==============================] - 16s - loss: 0.7652 - acc: 0.6375 - val_loss: 2.4761 - val_acc: 0.4571\n",
      "Epoch 4/13\n",
      "1305/1305 [==============================] - 16s - loss: 0.7223 - acc: 0.6682 - val_loss: 2.4709 - val_acc: 0.4540\n",
      "Epoch 5/13\n",
      "1305/1305 [==============================] - 15s - loss: 0.7320 - acc: 0.6590 - val_loss: 2.1281 - val_acc: 0.4785\n",
      "Epoch 6/13\n",
      "1305/1305 [==============================] - 15s - loss: 0.7262 - acc: 0.6498 - val_loss: 2.0405 - val_acc: 0.4877\n",
      "Epoch 7/13\n",
      "1305/1305 [==============================] - 16s - loss: 0.7097 - acc: 0.6782 - val_loss: 2.1819 - val_acc: 0.4448\n",
      "Epoch 8/13\n",
      "1305/1305 [==============================] - 16s - loss: 0.7064 - acc: 0.6613 - val_loss: 1.8438 - val_acc: 0.5092\n",
      "Epoch 9/13\n",
      "1305/1305 [==============================] - 16s - loss: 0.6297 - acc: 0.6996 - val_loss: 1.9055 - val_acc: 0.5153\n",
      "Epoch 10/13\n",
      "1305/1305 [==============================] - 16s - loss: 0.6634 - acc: 0.6851 - val_loss: 1.8556 - val_acc: 0.5307\n",
      "Epoch 11/13\n",
      "1305/1305 [==============================] - 16s - loss: 0.6812 - acc: 0.6904 - val_loss: 1.7568 - val_acc: 0.5307\n",
      "Epoch 12/13\n",
      "1305/1305 [==============================] - 16s - loss: 0.6636 - acc: 0.7019 - val_loss: 1.4817 - val_acc: 0.5307\n",
      "Epoch 13/13\n",
      "1305/1305 [==============================] - 16s - loss: 0.6309 - acc: 0.7096 - val_loss: 1.5172 - val_acc: 0.5706\n",
      "Found 3176 images belonging to 2 classes.\n",
      "3176/3176 [==============================] - 4s     \n",
      "\n",
      "Loss:1.35316266499, Accuracy:0.625\n",
      "validation set:  72\n",
      "Train set:  289\n",
      "Found 277 images belonging to 2 classes.\n",
      "Found 72 images belonging to 2 classes.\n",
      "Epoch 1/2\n",
      "277/277 [==============================] - 5s - loss: 1.0260 - acc: 0.5054 - val_loss: 0.7714 - val_acc: 0.5833\n",
      "Epoch 2/2\n",
      "277/277 [==============================] - 2s - loss: 0.9293 - acc: 0.5379 - val_loss: 0.9185 - val_acc: 0.5556\n",
      "Epoch 1/13\n",
      "277/277 [==============================] - 4s - loss: 0.9777 - acc: 0.5668 - val_loss: 0.7525 - val_acc: 0.5694\n",
      "Epoch 2/13\n",
      "277/277 [==============================] - 2s - loss: 0.8743 - acc: 0.6065 - val_loss: 0.7485 - val_acc: 0.5000\n",
      "Epoch 3/13\n",
      "277/277 [==============================] - 4s - loss: 0.8947 - acc: 0.5884 - val_loss: 0.6644 - val_acc: 0.5694\n",
      "Epoch 4/13\n",
      "277/277 [==============================] - 2s - loss: 0.7952 - acc: 0.6354 - val_loss: 0.6882 - val_acc: 0.5972\n",
      "Epoch 5/13\n",
      "277/277 [==============================] - 4s - loss: 0.8564 - acc: 0.6101 - val_loss: 0.7153 - val_acc: 0.5694\n",
      "Epoch 6/13\n",
      "277/277 [==============================] - 2s - loss: 0.8512 - acc: 0.6318 - val_loss: 0.7126 - val_acc: 0.5556\n",
      "Epoch 7/13\n",
      "277/277 [==============================] - 4s - loss: 0.7258 - acc: 0.6534 - val_loss: 0.7195 - val_acc: 0.5556\n",
      "Epoch 8/13\n",
      "277/277 [==============================] - 2s - loss: 0.6633 - acc: 0.7040 - val_loss: 0.7285 - val_acc: 0.5972\n",
      "Epoch 9/13\n",
      "277/277 [==============================] - 3s - loss: 0.7548 - acc: 0.6534 - val_loss: 0.8101 - val_acc: 0.5556\n",
      "Epoch 10/13\n",
      "277/277 [==============================] - 2s - loss: 0.6730 - acc: 0.6787 - val_loss: 0.8894 - val_acc: 0.5278\n",
      "Epoch 11/13\n",
      "277/277 [==============================] - 3s - loss: 0.6281 - acc: 0.7004 - val_loss: 0.8378 - val_acc: 0.5694\n",
      "Epoch 12/13\n",
      "277/277 [==============================] - 3s - loss: 0.6377 - acc: 0.7112 - val_loss: 0.8818 - val_acc: 0.5972\n",
      "Epoch 13/13\n",
      "277/277 [==============================] - 3s - loss: 0.5568 - acc: 0.7473 - val_loss: 0.8505 - val_acc: 0.5694\n",
      "Found 3176 images belonging to 2 classes.\n",
      "3168/3176 [============================>.] - ETA: 0s\n",
      "Loss:1.13933581027, Accuracy:0.46379093199\n",
      "validation set:  48\n",
      "Train set:  195\n",
      "Found 195 images belonging to 2 classes.\n",
      "Found 48 images belonging to 2 classes.\n",
      "Epoch 1/2\n",
      "195/195 [==============================] - 3s - loss: 1.1030 - acc: 0.4615 - val_loss: 0.8938 - val_acc: 0.5000\n",
      "Epoch 2/2\n",
      "195/195 [==============================] - 2s - loss: 1.1173 - acc: 0.4872 - val_loss: 0.7354 - val_acc: 0.5208\n",
      "Epoch 1/13\n",
      "195/195 [==============================] - 4s - loss: 0.7965 - acc: 0.6154 - val_loss: 0.7063 - val_acc: 0.5417\n",
      "Epoch 2/13\n",
      "195/195 [==============================] - 2s - loss: 0.9060 - acc: 0.6000 - val_loss: 0.6753 - val_acc: 0.5625\n",
      "Epoch 3/13\n",
      "195/195 [==============================] - 2s - loss: 0.8889 - acc: 0.5846 - val_loss: 0.7026 - val_acc: 0.5208\n",
      "Epoch 4/13\n",
      "195/195 [==============================] - 2s - loss: 0.7834 - acc: 0.6462 - val_loss: 0.7522 - val_acc: 0.5625\n",
      "Epoch 5/13\n",
      "195/195 [==============================] - 2s - loss: 0.7613 - acc: 0.6308 - val_loss: 0.7337 - val_acc: 0.5833\n",
      "Epoch 6/13\n",
      "195/195 [==============================] - 2s - loss: 0.8348 - acc: 0.5744 - val_loss: 0.7501 - val_acc: 0.5625\n",
      "Epoch 7/13\n",
      "195/195 [==============================] - 2s - loss: 0.8134 - acc: 0.6308 - val_loss: 0.8494 - val_acc: 0.5625\n",
      "Epoch 8/13\n",
      "195/195 [==============================] - 2s - loss: 0.5694 - acc: 0.6974 - val_loss: 0.9186 - val_acc: 0.5625\n",
      "Epoch 9/13\n",
      "195/195 [==============================] - 2s - loss: 0.8040 - acc: 0.6103 - val_loss: 0.8360 - val_acc: 0.5625\n",
      "Epoch 10/13\n",
      "195/195 [==============================] - 2s - loss: 0.8009 - acc: 0.7026 - val_loss: 0.8323 - val_acc: 0.5625\n",
      "Epoch 11/13\n",
      "195/195 [==============================] - 2s - loss: 0.6229 - acc: 0.6718 - val_loss: 0.8932 - val_acc: 0.5625\n",
      "Epoch 12/13\n",
      "195/195 [==============================] - 2s - loss: 0.6563 - acc: 0.6564 - val_loss: 0.8690 - val_acc: 0.5625\n",
      "Epoch 13/13\n",
      "195/195 [==============================] - 2s - loss: 0.5794 - acc: 0.7385 - val_loss: 0.8339 - val_acc: 0.5625\n",
      "Found 3176 images belonging to 2 classes.\n",
      "3168/3176 [============================>.] - ETA: 0s\n",
      "Loss:0.843219859624, Accuracy:0.538413098237\n",
      "validation set:  148\n",
      "Train set:  592\n",
      "Found 592 images belonging to 2 classes.\n",
      "Found 148 images belonging to 2 classes.\n",
      "Epoch 1/2\n",
      "592/592 [==============================] - 9s - loss: 1.1482 - acc: 0.5084 - val_loss: 0.8886 - val_acc: 0.4797\n",
      "Epoch 2/2\n",
      "592/592 [==============================] - 6s - loss: 0.8689 - acc: 0.6030 - val_loss: 0.8507 - val_acc: 0.5135\n",
      "Epoch 1/13\n",
      "592/592 [==============================] - 9s - loss: 0.9449 - acc: 0.5726 - val_loss: 0.7142 - val_acc: 0.4797\n",
      "Epoch 2/13\n",
      "592/592 [==============================] - 7s - loss: 0.9003 - acc: 0.5963 - val_loss: 0.7046 - val_acc: 0.5473\n",
      "Epoch 3/13\n",
      "592/592 [==============================] - 7s - loss: 0.8504 - acc: 0.6115 - val_loss: 0.7888 - val_acc: 0.4797\n",
      "Epoch 4/13\n",
      "592/592 [==============================] - 6s - loss: 0.8183 - acc: 0.6233 - val_loss: 0.8865 - val_acc: 0.4662\n",
      "Epoch 5/13\n",
      "592/592 [==============================] - 7s - loss: 0.7825 - acc: 0.6520 - val_loss: 0.8209 - val_acc: 0.5000\n",
      "Epoch 6/13\n",
      "592/592 [==============================] - 7s - loss: 0.7743 - acc: 0.6470 - val_loss: 0.8490 - val_acc: 0.5000\n",
      "Epoch 7/13\n",
      "592/592 [==============================] - 6s - loss: 0.7326 - acc: 0.6453 - val_loss: 0.8688 - val_acc: 0.5405\n",
      "Epoch 8/13\n",
      "592/592 [==============================] - 7s - loss: 0.7224 - acc: 0.6706 - val_loss: 0.9515 - val_acc: 0.4730\n",
      "Epoch 9/13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "592/592 [==============================] - 6s - loss: 0.7456 - acc: 0.6520 - val_loss: 0.8846 - val_acc: 0.5135\n",
      "Epoch 10/13\n",
      "592/592 [==============================] - 6s - loss: 0.7533 - acc: 0.6672 - val_loss: 0.8341 - val_acc: 0.5068\n",
      "Epoch 11/13\n",
      "592/592 [==============================] - 7s - loss: 0.7024 - acc: 0.6453 - val_loss: 0.8796 - val_acc: 0.5270\n",
      "Epoch 12/13\n",
      "592/592 [==============================] - 6s - loss: 0.7118 - acc: 0.6622 - val_loss: 0.7695 - val_acc: 0.5676\n",
      "Epoch 13/13\n",
      "592/592 [==============================] - 7s - loss: 0.7030 - acc: 0.6723 - val_loss: 0.7958 - val_acc: 0.6216\n",
      "Found 3176 images belonging to 2 classes.\n",
      "3168/3176 [============================>.] - ETA: 0s\n",
      "Loss:0.792986744597, Accuracy:0.563916876574\n",
      "validation set:  268\n",
      "Train set:  1075\n",
      "Found 1075 images belonging to 2 classes.\n",
      "Found 268 images belonging to 2 classes.\n",
      "Epoch 1/2\n",
      "1075/1075 [==============================] - 16s - loss: 0.9345 - acc: 0.5777 - val_loss: 0.7413 - val_acc: 0.5299\n",
      "Epoch 2/2\n",
      "1075/1075 [==============================] - 12s - loss: 0.8437 - acc: 0.6233 - val_loss: 1.1788 - val_acc: 0.5149\n",
      "Epoch 1/13\n",
      "1075/1075 [==============================] - 16s - loss: 0.8241 - acc: 0.6223 - val_loss: 1.3984 - val_acc: 0.5746\n",
      "Epoch 2/13\n",
      "1075/1075 [==============================] - 13s - loss: 0.8385 - acc: 0.6344 - val_loss: 1.6995 - val_acc: 0.5373\n",
      "Epoch 3/13\n",
      "1075/1075 [==============================] - 12s - loss: 0.8267 - acc: 0.6391 - val_loss: 1.6569 - val_acc: 0.5000\n",
      "Epoch 4/13\n",
      "1075/1075 [==============================] - 12s - loss: 0.7373 - acc: 0.6512 - val_loss: 1.9100 - val_acc: 0.5149\n",
      "Epoch 5/13\n",
      "1075/1075 [==============================] - 13s - loss: 0.8132 - acc: 0.6344 - val_loss: 1.7862 - val_acc: 0.5373\n",
      "Epoch 6/13\n",
      "1075/1075 [==============================] - 13s - loss: 0.7628 - acc: 0.6567 - val_loss: 1.9129 - val_acc: 0.5224\n",
      "Epoch 7/13\n",
      "1075/1075 [==============================] - 13s - loss: 0.7806 - acc: 0.6595 - val_loss: 1.9230 - val_acc: 0.5037\n",
      "Epoch 8/13\n",
      "1075/1075 [==============================] - 12s - loss: 0.7587 - acc: 0.6419 - val_loss: 1.9863 - val_acc: 0.5224\n",
      "Epoch 9/13\n",
      "1075/1075 [==============================] - 13s - loss: 0.6730 - acc: 0.6874 - val_loss: 1.8792 - val_acc: 0.5149\n",
      "Epoch 10/13\n",
      "1075/1075 [==============================] - 13s - loss: 0.7064 - acc: 0.6558 - val_loss: 1.7545 - val_acc: 0.5410\n",
      "Epoch 11/13\n",
      "1075/1075 [==============================] - 13s - loss: 0.6622 - acc: 0.7005 - val_loss: 1.7230 - val_acc: 0.4739\n",
      "Epoch 12/13\n",
      "1075/1075 [==============================] - 13s - loss: 0.6177 - acc: 0.7153 - val_loss: 1.6634 - val_acc: 0.5373\n",
      "Epoch 13/13\n",
      "1075/1075 [==============================] - 12s - loss: 0.6336 - acc: 0.6921 - val_loss: 1.4316 - val_acc: 0.5410\n",
      "Found 3176 images belonging to 2 classes.\n",
      "3168/3176 [============================>.] - ETA: 0s\n",
      "Loss:1.57248891425, Accuracy:0.544080604534\n",
      "validation set:  44\n",
      "Train set:  176\n",
      "Found 171 images belonging to 2 classes.\n",
      "Found 44 images belonging to 2 classes.\n",
      "Epoch 1/2\n",
      "171/171 [==============================] - 3s - loss: 1.1553 - acc: 0.4795 - val_loss: 1.8474 - val_acc: 0.7045\n",
      "Epoch 2/2\n",
      "171/171 [==============================] - 2s - loss: 0.7852 - acc: 0.6491 - val_loss: 0.6611 - val_acc: 0.5682\n",
      "Epoch 1/13\n",
      "171/171 [==============================] - 3s - loss: 0.8320 - acc: 0.6316 - val_loss: 1.4261 - val_acc: 0.2955\n",
      "Epoch 2/13\n",
      "171/171 [==============================] - 2s - loss: 0.7517 - acc: 0.6316 - val_loss: 1.0809 - val_acc: 0.2955\n",
      "Epoch 3/13\n",
      "171/171 [==============================] - 2s - loss: 0.5825 - acc: 0.7193 - val_loss: 0.6981 - val_acc: 0.4773\n",
      "Epoch 4/13\n",
      "171/171 [==============================] - 1s - loss: 0.8351 - acc: 0.6433 - val_loss: 0.6847 - val_acc: 0.5227\n",
      "Epoch 5/13\n",
      "171/171 [==============================] - 2s - loss: 0.6200 - acc: 0.7076 - val_loss: 0.8017 - val_acc: 0.4091\n",
      "Epoch 6/13\n",
      "171/171 [==============================] - 1s - loss: 0.7727 - acc: 0.6374 - val_loss: 1.0302 - val_acc: 0.3409\n",
      "Epoch 7/13\n",
      "171/171 [==============================] - 2s - loss: 0.6100 - acc: 0.7135 - val_loss: 1.1841 - val_acc: 0.2955\n",
      "Epoch 8/13\n",
      "171/171 [==============================] - 1s - loss: 0.6677 - acc: 0.6842 - val_loss: 1.1953 - val_acc: 0.2955\n",
      "Epoch 9/13\n",
      "107/171 [=================>............] - ETA: 0s - loss: 0.5319 - acc: 0.7570"
     ]
    }
   ],
   "source": [
    "%cd $data_dir/train\n",
    "\n",
    "nepoch = 15\n",
    "batch_size = 32\n",
    "train_size = 100\n",
    "total_train_size = 100\n",
    "valid_size = int(math.floor(.2 * train_size))\n",
    "#print('sample size: {}'.format(train_size + valid_size))\n",
    "tr_model = None\n",
    "\n",
    "i=0\n",
    "for i in range(20):\n",
    "    #print(\"Train Size:{}\".format(train_size))\n",
    "    #print(\"Valid Size:{}\".format(valid_size))\n",
    "    tr_model = fit(i, tr_model, path, results_path, nepoch, batch_size, train_size, valid_size)\n",
    "\n",
    "    model = None\n",
    "    model = get_test_model() \n",
    "    model.load_weights(results_path+'/ft_' + str(train_size) + '.e' + str(nepoch))\n",
    "    probs, test_batches = predict(path, model)\n",
    "\n",
    "    retrain_idx = np.unique(np.where(np.logical_and(probs >=0.3, probs<=0.7))[0])\n",
    "    retrain_set = [test_batches.filenames[i] for i in retrain_idx]\n",
    "\n",
    "    os.chdir(path + 'test')\n",
    "    ndog = sum('dog' in name for name in retrain_set)\n",
    "    ncat =  sum('cat' in name for name in retrain_set)\n",
    "    limit = min(ncat, ndog)\n",
    "\n",
    "    # print(\"Total Retrain Images: {}\".format(limit*2))\n",
    "\n",
    "    #print('Images to be retrained:{}'.format(limit*2))\n",
    "    #print('Dogs:{0}, Cats:{1}, Total:{2}'.format(ndog, ncat, ndog+ncat))\n",
    "\n",
    "    # # # #remove all train/validation images\n",
    "    # print(\"Total Retrain Images: {}\".format(limit*2))\n",
    "    #move existing training data to the store\n",
    "    adjust_prev_data_sample()\n",
    "    train_size, valid_size, copied_images = move_to_train(retrain_set, limit)\n",
    "    total_train_size += train_size\n",
    "    refil_test(copied_images)\n",
    "\n",
    "print('Total No. of Images:{}'.format(total_train_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# cats_copied=5\n",
    "# dogs_copied=5\n",
    "# for fil in retrain_set:\n",
    "#     #print(\"Retrain File: {}\".format(fil))\n",
    "#     fil_cpy = fil[fil.find('/')+1:]\n",
    "#     if 'cat' in fil and cats_copied <= to_be_copied:\n",
    "#         #print(os.path.join(path + \"test/cats/\"+ fil_cpy))\n",
    "#         #print(os.path.join(path + \"test_copy/\"+ fil_cpy))\n",
    "#         copyfile(os.path.join(path + \"test/cats/\"+ fil_cpy), os.path.join(path + \"test_copy/\"+ fil_cpy))\n",
    "#         cats_copied+=1\n",
    "#     elif 'dog' in fil and dogs_copied <= to_be_copied:\n",
    "#         #print(os.path.join(path + \"test/dogs/\"+ fil_cpy))\n",
    "#         #print(os.path.join(path + \"test_copy/\"+ fil_cpy))              \n",
    "#         copyfile(os.path.join(path + \"test/dogs/\"+ fil_cpy), os.path.join(path + \"test_copy/\"+ fil_cpy))\n",
    "#         dogs_copied+=1\n",
    "    \n",
    "#     if cats_copied >= to_be_copied and dogs_copied >= to_be_copied:\n",
    "#         #print(len(os.listdir(path + \"test_copy/\")))\n",
    "#         print(cats_copied, dogs_copied, to_be_copied)\n",
    "#         break\n",
    "\n",
    "# print(\"copy done.\")\n",
    "\n",
    "#move to validation directory\n",
    "#replenish validation set\n",
    "\n",
    "#move_to_test(cats_copied+dogs_copied)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "#     for fil in retrain_set:\n",
    "#         fil_cpy = fil[fil.find('/')+1:]\n",
    "#         if 'cat' in fil and cats_copied <= limit:\n",
    "#             #print(os.path.join(path + \"test/cats/\"+ fil_cpy))\n",
    "#             #print(os.path.join(path + \"test_copy/\"+ fil_cpy))\n",
    "#             copyfile(os.path.join(path + \"test/cats/\"+ fil_cpy), os.path.join(path + \"test_copy/\"+ fil_cpy))\n",
    "#             cats_copied+=1\n",
    "#         elif 'dog' in fil and dogs_copied <= limit:\n",
    "#             #print(os.path.join(path + \"test/dogs/\"+ fil_cpy))\n",
    "#             #print(os.path.join(path + \"test_copy/\"+ fil_cpy))              \n",
    "#             copyfile(os.path.join(path + \"test/dogs/\"+ fil_cpy), os.path.join(path + \"test_copy/\"+ fil_cpy))\n",
    "#             dogs_copied+=1\n",
    "\n",
    "#         if cats_copied >= limit and dogs_copied >= limit:\n",
    "#             #print(len(os.listdir(path + \"test_copy/\")))\n",
    "#             print(cats_copied, dogs_copied, limit)\n",
    "#             break\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# def restore_backup(source, destination):\n",
    "#     %mv $source/cat*.jpg $destination/cats/\n",
    "#     %mv $source/dog*.jpg $destination/dogs/\n",
    "    \n",
    "# def remove_to_backup(source, destination):\n",
    "#     %mv $source/* $destination/\n",
    "\n",
    "# remove_to_backup(path + 'train/cats/', path + 'train_bkup/')\n",
    "# remove_to_backup(path + 'train/dogs/', path + 'train_bkup/')\n",
    "\n",
    "#restore_backup(path + 'train_bkup', path + 'train')\n",
    "\n",
    "# nimages = cats_copied + dogs_copied\n",
    "# def move_to_test(nimages):\n",
    "#     os.chdir(\"/home/asaeed9/work/data/2cat/train\")\n",
    "#     g = glob('*.jpg')\n",
    "#     shuf = np.random.permutation(g)\n",
    "#     for i in range(nimages): os.rename(shuf[i], '../sample/test/' + shuf[i])\n",
    "#     os.chdir(\"../sample/test/\")\n",
    "#     move(\"cat*.jpg\")\n",
    "#     %mv ../sample/test/cat*.jpg ../sample/cats/\n",
    "#     %mv ../sample/test/dog*.jpg ../sample/dogs/    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#move_from_test(retrain_set, path, train_size, validation_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "model = get_test_model()\n",
    "loss_score = []\n",
    "accuracy_score = []\n",
    "training_size = [100]\n",
    "\n",
    "os.chdir(results_path)\n",
    "files = filter(os.path.isfile, os.listdir(results_path))\n",
    "files.sort(key=lambda x: os.path.getmtime(x))\n",
    "\n",
    "for f in files:\n",
    "    \n",
    "    print(\"\\nFile being processed: {}\".format(f))\n",
    "    \n",
    "    train_size = int(f[f.find('_')+1 : f.find('.')])\n",
    "    epoch = int(f[f.find('e')+1 : ])\n",
    "\n",
    "    model.load_weights(results_path+'/ft_' + str(train_size) + '.e' + str(epoch))\n",
    "    \n",
    "    gen_test = image.ImageDataGenerator()\n",
    "    test_batches = gen_test.flow_from_directory(path+'test', class_mode=None, target_size=(256,256), shuffle=False, batch_size=1)\n",
    "    test_data = np.concatenate([test_batches.next() for i in range(test_batches.nb_sample)])\n",
    "    test_labels = onehot(test_batches.classes)\n",
    "    score = model.evaluate(test_data, test_labels)\n",
    "    \n",
    "    loss_score.append(score[0])\n",
    "    accuracy_score.append(score[1])\n",
    "    \n",
    "    print(\"\\nLoss:{}, Accuracy:{}\".format(score[0], score[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "model = get_test_model()\n",
    "loss_score = []\n",
    "accuracy_score = []\n",
    "training_size = [100]\n",
    "\n",
    "os.chdir(results_path)\n",
    "files = filter(os.path.isfile, os.listdir(results_path))\n",
    "files.sort(key=lambda x: os.path.getmtime(x))\n",
    "\n",
    "for f in files:\n",
    "    \n",
    "    print(\"\\nFile being processed: {}\".format(f))\n",
    "    \n",
    "    train_size = int(f[f.find('_')+1 : f.find('.')])\n",
    "    epoch = int(f[f.find('e')+1 : ])\n",
    "\n",
    "    model.load_weights(results_path+'/ft_' + str(train_size) + '.e' + str(epoch))\n",
    "    \n",
    "    gen_test = image.ImageDataGenerator()\n",
    "    test_batches = gen_test.flow_from_directory(path+'test', class_mode=None, target_size=(256,256), shuffle=False, batch_size=1)\n",
    "    test_data = np.concatenate([test_batches.next() for i in range(test_batches.nb_sample)])\n",
    "    test_labels = onehot(test_batches.classes)\n",
    "    score = model.evaluate(test_data, test_labels)\n",
    "    \n",
    "    probs = model.predict(test_data)\n",
    "    \n",
    "    loss_score.append(score[0])\n",
    "    accuracy_score.append(score[1])\n",
    "    \n",
    "    print(\"\\nLoss:{}, Accuracy:{}\".format(score[0], score[1]))\n",
    "    print(\"\\nProbabilities:{}\".format(probs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "Image.open(test_path +  'cats/cat.6013.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#probs[:10]\n",
    "retrain_idx = np.where(np.logical_and(probs >=0.4, probs<=0.6))[0]\n",
    "retrain_set = [test_batches.filenames[i] for i in retrain_idx]\n",
    "\n",
    "\n",
    "\n",
    "retrain_set[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_model(tr_batches, epoch):\n",
    "    model = Sequential([\n",
    "            BatchNormalization(axis=1, input_shape=(3, 256,256)),\n",
    "            Convolution2D(32,3,3, activation='relu'),\n",
    "            BatchNormalization(axis=1),\n",
    "            MaxPooling2D((3,3)),\n",
    "            Dropout(0.2),\n",
    "#             Convolution2D(64,3,3, activation='relu'),\n",
    "#             BatchNormalization(axis=1),\n",
    "            #MaxPooling2D((3,3)),\n",
    "            Convolution2D(64,3,3, activation='relu'),\n",
    "            BatchNormalization(axis=1),\n",
    "            MaxPooling2D((3,3)),\n",
    "            Dropout(0.2),\n",
    "            Convolution2D(32,3,3, activation='relu'),\n",
    "            BatchNormalization(axis=1),\n",
    "            MaxPooling2D((3,3)),\n",
    "            Dropout(0.2),\n",
    "            Flatten(),\n",
    "            Dense(1024, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.2),\n",
    "            Dense(2, activation='softmax')\n",
    "        ])\n",
    "\n",
    "    model.compile(Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    #model.fit_generator(tr_batches, tr_batches.nb_sample, nb_epoch=2)\n",
    "    #model.optimizer.lr = 0.001\n",
    "    #model.fit_generator(tr_batches, tr_batches.nb_sample, nb_epoch=epoch - 2)\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model = get_train_model(new_data, nepoch)\n",
    "#model.load_weights(results_path + \"/ft_100.e50\")\n",
    "#model.fit_generator(new_data, len(new_data), nb_epoch = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
