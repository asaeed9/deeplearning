{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 1: GeForce GTX 980 Ti (CNMeM is disabled, cuDNN 5110)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import os, sys, cv2\n",
    "os.chdir(\"/home/asaeed9/work/deeplearning\")\n",
    "from theano.sandbox import cuda\n",
    "#cuda.use('gpu1')\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "from IPython.display import FileLink\n",
    "from keras.preprocessing import image, sequence\n",
    "from shutil import copyfile, move\n",
    "from random import shuffle\n",
    "\n",
    "####\n",
    "from keras.layers.convolutional import *\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir=\"/home/asaeed9/work/data/2cat_18112017\"\n",
    "data_dir_F = data_dir + \"/F/\" \n",
    "data_src=\"sample\"\n",
    "path=\"/home/asaeed9/work/data/2cat_18112017/\" + data_src + \"/\"\n",
    "results_path = \"/home/asaeed9/work/data/2cat_18112017/\"+ data_src + \"/results\"\n",
    "test_path = path + '/test/' #We use all the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_count(dir_path):\n",
    "    return len([name for name in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path,name))])\n",
    "\n",
    "def get_train_valid_size():\n",
    "        tr_count = get_file_count(path + \"/train/cats\") + get_file_count(path + \"/train/dogs\")\n",
    "        vl_count = get_file_count(path + \"/valid/cats\") + get_file_count(path + \"/valid/dogs\")\n",
    "        \n",
    "        return tr_count, vl_count \n",
    "\n",
    "def move_files(src_path, dest_path, pattern, n):\n",
    "    \n",
    "#     print(src_path)\n",
    "#     print(dest_path)\n",
    "#     print(pattern)\n",
    "#     print(n)\n",
    "    \n",
    "    os.chdir(src_path)\n",
    "    try:\n",
    "        if type(pattern) is np.ndarray:\n",
    "                for img in pattern:\n",
    "                    move(os.path.join(src_path+'/', os.path.basename(img)), os.path.join(dest_path+'/',os.path.basename(img)))\n",
    "        else:\n",
    "            if n:\n",
    "                g = glob(src_path + '/' + pattern)\n",
    "                shuf = np.random.permutation(g)\n",
    "                for i in range(n):\n",
    "                    move(os.path.join(src_path+'/', os.path.basename(shuf[i])),os.path.join(dest_path+'/',os.path.basename(shuf[i])))\n",
    "            else:\n",
    "                for file in glob(src_path + '/' + pattern):\n",
    "                    move(os.path.join(src_path+'/', os.path.basename(file)), os.path.join(dest_path+'/',os.path.basename(file)))\n",
    "\n",
    "    except IOError, e:\n",
    "            print (\"Unable to move file - {}\".format(e))\n",
    "            \n",
    "def prep_experiment2(block, src, dest):\n",
    "    #move_files(src + \"/cats\" ,dest, \"*.jpg\",None)\n",
    "    #move_files(src + \"/dogs\" ,dest, \"*.jpg\", None) \n",
    "    move_random_data(None, path + '/train/', data_dir_F, False)\n",
    "    \n",
    "    move_block_images(block['train'], data_dir_F, path + '/train/', False)\n",
    "    move_block_images(block['valid'], data_dir_F, path + '/valid/', False)\n",
    "    \n",
    "    move_random_data(None, data_dir_F, path + '/' + dest + '/', False)\n",
    "\n",
    "def move_random_data(n, src_dir, dest_path, clean):\n",
    "    if clean:\n",
    "        move_files(dest_path + \"/cats\" ,src_dir, \"*.jpg\",None)\n",
    "        move_files(dest_path + \"/dogs\" ,src_dir, \"*.jpg\", None)\n",
    "\n",
    "    move_files(src_dir, dest_path + \"/cats\",\"cat*.jpg\", n)\n",
    "    move_files(src_dir, dest_path + \"/dogs\",\"dog*.jpg\", n)\n",
    "\n",
    "def move_block_images(images, src_dir, dest_path, clean):\n",
    "    if clean:\n",
    "        move_files(dest_path + \"/cats\" ,src_dir, \"*.jpg\",None)\n",
    "        move_files(dest_path + \"/dogs\" ,src_dir, \"*.jpg\", None)\n",
    "\n",
    "    move_files(src_dir, dest_path + \"/cats\",images['cats'] , None)\n",
    "    move_files(src_dir, dest_path + \"/dogs\",images['dogs'], None)\n",
    "\n",
    "def get_data_blocks(src_dir):\n",
    "    \n",
    "    os.chdir(src_dir)\n",
    "    r_set = []\n",
    "    batch_size = 250\n",
    "\n",
    "    g_cat = glob(src_dir + \"/cat*.jpg\")\n",
    "    shuf_cats = np.random.permutation(g_cat)\n",
    "\n",
    "    g_dog = glob(src_dir + \"/dog*.jpg\")\n",
    "    shuf_dogs = np.random.permutation(g_dog)\n",
    "\n",
    "    rk_cats = int(math.floor(len(shuf_cats)/batch_size))\n",
    "    rk_dogs = int(math.floor(len(shuf_dogs)/batch_size))\n",
    "\n",
    "    if rk_cats != rk_dogs:\n",
    "        print(\"cats/dogs set isn't equal.\")\n",
    "\n",
    "    for index in range(rk_cats): \n",
    "        \n",
    "        valid_size = int(math.floor(.2 * batch_size))\n",
    "        train_size = batch_size - valid_size\n",
    "\n",
    "        block_start = batch_size * index\n",
    "        block_end = batch_size * (index + 1)\n",
    "        #print(block_start, block_end)\n",
    "        r_set.append({'train': {'cats':shuf_cats[block_start:block_start + train_size], 'dogs':shuf_dogs[block_start:block_start+train_size]}, \n",
    "                      'valid':{'cats':shuf_cats[block_start + train_size:block_end], 'dogs':shuf_dogs[block_start + train_size:block_end]}})\n",
    "    \n",
    "    return r_set        \n",
    "        \n",
    "def fit(old_model, path, results_path, nepoch, batch_size, train_size, valid_size):\n",
    "    gen_t = image.ImageDataGenerator(rotation_range=15, height_shift_range=0.05,\n",
    "                shear_range=0.1, channel_shift_range=20, width_shift_range=0.1)\n",
    "\n",
    "    tr_batches = gen_t.flow_from_directory(path + 'train', batch_size=batch_size)\n",
    "    val_batches = gen_t.flow_from_directory(path + 'valid', class_mode='categorical', \n",
    "                                            shuffle=True, batch_size=batch_size * 2)\n",
    "    model = None\n",
    "    if old_model:\n",
    "        model = train_model(old_model, tr_batches, val_batches, nepoch)\n",
    "    else:\n",
    "        print(\"None Model...\")\n",
    "        model = train_model(None, tr_batches, val_batches, nepoch)\n",
    "\n",
    "    #model.summary()\n",
    "    \n",
    "    #model.save_weights(results_path+ '/' + 'ft_' + str(train_size) + '.e' + str(nepoch))\n",
    "    last_file_timestamp = '{:%Y%m%d%H%M%S}'.format(datetime.datetime.now())\n",
    "    #print('File Time Stamp:{}'.format(last_file_timestamp))\n",
    "    model.save_weights(results_path+'/ft_{}'.format(last_file_timestamp))\n",
    "    \n",
    "    print(\"saved file...\")\n",
    "    \n",
    "    #model.load_weights(results_path+'/ft_{}'.format(last_file_timestamp))  \n",
    "\n",
    "def get_train_model():\n",
    "        model = Sequential([\n",
    "                BatchNormalization(axis=1, input_shape=(3, 256,256)),\n",
    "                Convolution2D(32,3,3, activation='relu'),\n",
    "                BatchNormalization(axis=1),\n",
    "                MaxPooling2D((3,3)),\n",
    "                Dropout(0.2),\n",
    "    #             Convolution2D(64,3,3, activation='relu'),\n",
    "    #             BatchNormalization(axis=1),\n",
    "                #MaxPooling2D((3,3)),\n",
    "                Convolution2D(64,3,3, activation='relu'),\n",
    "                BatchNormalization(axis=1),\n",
    "                MaxPooling2D((3,3)),\n",
    "                Dropout(0.2),\n",
    "                Convolution2D(32,3,3, activation='relu'),\n",
    "                BatchNormalization(axis=1),\n",
    "                MaxPooling2D((3,3)),\n",
    "                Dropout(0.2),\n",
    "                Flatten(),\n",
    "                Dense(1024, activation='relu'),\n",
    "                BatchNormalization(),\n",
    "                Dropout(0.2),\n",
    "                Dense(2, activation='softmax')\n",
    "            ])\n",
    "        model.compile(Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        return model\n",
    "\n",
    "def train_model(model, tr_batches, val_batches, epoch):\n",
    "    if not model:\n",
    "        model = Sequential([\n",
    "                BatchNormalization(axis=1, input_shape=(3, 256,256)),\n",
    "                Convolution2D(32,3,3, activation='relu'),\n",
    "                BatchNormalization(axis=1),\n",
    "                MaxPooling2D((3,3)),\n",
    "                Dropout(0.2),\n",
    "    #             Convolution2D(64,3,3, activation='relu'),\n",
    "    #             BatchNormalization(axis=1),\n",
    "                #MaxPooling2D((3,3)),\n",
    "                Convolution2D(64,3,3, activation='relu'),\n",
    "                BatchNormalization(axis=1),\n",
    "                MaxPooling2D((3,3)),\n",
    "                Dropout(0.2),\n",
    "                Convolution2D(32,3,3, activation='relu'),\n",
    "                BatchNormalization(axis=1),\n",
    "                MaxPooling2D((3,3)),\n",
    "                Dropout(0.2),\n",
    "                Flatten(),\n",
    "                Dense(1024, activation='relu'),\n",
    "                BatchNormalization(),\n",
    "                Dropout(0.2),\n",
    "                Dense(2, activation='softmax')\n",
    "            ])\n",
    "        model.compile(Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.fit_generator(tr_batches, tr_batches.nb_sample, nb_epoch=2, validation_data=val_batches,\n",
    "                     nb_val_samples=val_batches.nb_sample)\n",
    "\n",
    "    model.optimizer.lr = 0.1\n",
    "    model.fit_generator(tr_batches, tr_batches.nb_sample, nb_epoch=1, validation_data=val_batches,\n",
    "                     nb_val_samples=val_batches.nb_sample)\n",
    "\n",
    "    model.optimizer.lr = 0.001\n",
    "    model.fit_generator(tr_batches, tr_batches.nb_sample, nb_epoch=epoch - 3, validation_data=val_batches,\n",
    "                     nb_val_samples=val_batches.nb_sample)\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_test_model():\n",
    "    model = Sequential([\n",
    "                BatchNormalization(axis=1, input_shape=(3,256,256)),\n",
    "                Convolution2D(32,3,3, activation='relu'),\n",
    "                BatchNormalization(axis=1),\n",
    "                MaxPooling2D((3,3)),\n",
    "    #             Convolution2D(64,3,3, activation='relu'),\n",
    "    #             BatchNormalization(axis=1),\n",
    "                #MaxPooling2D((3,3)),\n",
    "                Convolution2D(64,3,3, activation='relu'),\n",
    "                BatchNormalization(axis=1),\n",
    "                MaxPooling2D((3,3)),\n",
    "                Convolution2D(32,3,3, activation='relu'),\n",
    "                BatchNormalization(axis=1),\n",
    "                MaxPooling2D((3,3)),\n",
    "                Flatten(),\n",
    "                Dense(1024, activation='relu'),\n",
    "                BatchNormalization(),\n",
    "                Dense(2, activation='softmax')\n",
    "            ])\n",
    "\n",
    "    model.compile(Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def run_experiment1(block, nepoch, batch_size, tr_model):\n",
    "    \n",
    "    move_block_images(block['train'], data_dir_F, path + '/train/', False)\n",
    "    move_block_images(block['valid'], data_dir_F, path + '/valid/', False)\n",
    "\n",
    "    train_size, valid_size = get_train_valid_size()\n",
    "    tr_model,file_timestamp = fit(tr_model, path, results_path, nepoch, batch_size, train_size, valid_size)\n",
    "\n",
    "    model = None\n",
    "    model = get_test_model()\n",
    "    #model.load_weights(results_path+'/ft_' + str(train_size) + '.e' + str(nepoch))\n",
    "    #print('{0}/ft_{1}'.format(last_file_timestamp))\n",
    "    #print('Last File Timestamp- before loading:{}'.format(file_timestamp))\n",
    "    model.load_weights(results_path+'/ft_{}'.format(file_timestamp))\n",
    "\n",
    "    print(\"\\nVerification on Experiment#1 Test set.\")\n",
    "    probs, test_batches,loss, accuracy = predict(path, model, \"test\")\n",
    "    #print('\\nExperiment#1 Accuracy:{}'.format(accuracy))\n",
    "    #print('Experiment#2 Loss:{}'.format(loss))\n",
    "    \n",
    "    return probs, test_batches, loss, accuracy, train_size, valid_size\n",
    "\n",
    "    \n",
    "def move_samples_imbalance(retrain_set, dest_path):\n",
    "    for fil in retrain_set:\n",
    "        fil_cpy = fil[fil.find('/')+1:]  \n",
    "        \n",
    "        if \"cat\" in fil_cpy:\n",
    "            os.rename(os.path.join(path + \"unlabel/cats/\"+ fil_cpy), os.path.join(path + dest_path + \"/cats/\"+ fil_cpy))\n",
    "        else:\n",
    "            os.rename(os.path.join(path + \"unlabel/dogs/\"+ fil_cpy), os.path.join(path + dest_path + \"/dogs/\" + fil_cpy))\n",
    "\n",
    "    return len(retrain_set) #images copied \n",
    "\n",
    "def run_experiment2(block, nepoch, batch_size, tr_model):    \n",
    "    retrain_size = 500\n",
    "    \n",
    "    train_size, valid_size = get_train_valid_size()\n",
    "    tr_model,file_timestamp = fit(tr_model, path, results_path, nepoch, batch_size, train_size, valid_size)\n",
    "    \n",
    "    print(\"After fitting a model.\")\n",
    "    \n",
    "    model = None\n",
    "    model = get_test_model()\n",
    "    #model.load_weights(results_path+'/ft_' + str(train_size) + '.e' + str(nepoch))\n",
    "    #print('{0}/ft_{1}'.format(last_file_timestamp))\n",
    "    #print('Last File Timestamp- before loading:{}'.format(file_timestamp))\n",
    "    model.load_weights(results_path+'/ft_{}'.format(file_timestamp))\n",
    "\n",
    "    print(\"\\nVerification on Unlabel set.\")\n",
    "    probs, test_batches,loss, accuracy = predict(path, model, \"unlabel\")\n",
    "    print('\\nUnlabel Accuracy:{}'.format(accuracy))\n",
    "    print('Unlabel Loss:{}'.format(loss))\n",
    "\n",
    "    #get the top retrain_size most confused images\n",
    "\n",
    "    retrain_idx = np.argsort(abs(0.5 - probs[:, 1]))[:retrain_size]\n",
    "    retrain_set = [test_batches.filenames[i] for i in retrain_idx]\n",
    "\n",
    "    #print('Retrain Set Length:{}'.format(len(retrain_set)))\n",
    "\n",
    "    ndog = sum('dog' in name for name in retrain_set)\n",
    "    ncat =  sum('cat' in name for name in retrain_set)\n",
    "    valid_dog = int(math.floor(.2*ndog))\n",
    "    valid_cat = int(math.floor(.2*ncat))\n",
    "    train_dog = ndog - valid_dog\n",
    "    train_cat = ncat - valid_cat\n",
    "\n",
    "    print(\"Total Dogs: {}, Total Cats: {}\".format(ndog, ncat))\n",
    "    print(\"Valid Dog: {}, Valid Cat: {}\".format(valid_dog, valid_cat))\n",
    "    \n",
    "    print(type(retrain_set))\n",
    "    \n",
    "    print(retrain_set)\n",
    "\n",
    "    # os.chdir(path + 'unlabel')\n",
    "    #move existing training data to the store\n",
    "    #adjust_prev_data_sample(\"used_train\")\n",
    "    #train_size, valid_size, copied_images = move_to_train(retrain_set)\n",
    "    #copied_images = move_samples_imbalance(retrain_set, \"train\")\n",
    "    #refil_unlabel(ndog, ncat, \"unlabel\") #move removed dogs and cats to unlabel segment again\n",
    "    #refil_unlabel(valid_dog, valid_cat, \"valid\") #catch up validation set to training, to maintain 80/20 distribution\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Test set by copying 4000 images from F set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Divide the images into set R1, R2, R3...R_(21000/500)\n",
    "build R set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First batch of images need to be moved to training directory for the Experiment#1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400 images belonging to 2 classes.\n",
      "Found 100 images belonging to 2 classes.\n",
      "None Model...\n",
      "Epoch 1/2\n",
      "400/400 [==============================] - 6s - loss: 1.1050 - acc: 0.5125 - val_loss: 4.8678 - val_acc: 0.5000\n",
      "Epoch 2/2\n",
      "400/400 [==============================] - 4s - loss: 0.9460 - acc: 0.5425 - val_loss: 1.8523 - val_acc: 0.5000\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 6s - loss: 0.9069 - acc: 0.5850 - val_loss: 0.7772 - val_acc: 0.5200\n",
      "Epoch 1/2\n",
      "400/400 [==============================] - 6s - loss: 0.9723 - acc: 0.5750 - val_loss: 0.7186 - val_acc: 0.5500\n",
      "Epoch 2/2\n",
      "400/400 [==============================] - 4s - loss: 0.7983 - acc: 0.6250 - val_loss: 0.7296 - val_acc: 0.5000\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-03f45036aabf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m#prep_experiment2(r_set[0], data_dir_F, 'unlabel')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m#for block in r_set:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-56560150c726>\u001b[0m in \u001b[0;36mrun_experiment2\u001b[0;34m(block, nepoch, batch_size, tr_model)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_train_valid_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m     \u001b[0mtr_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfile_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "#os.chdir(\"/home/asaeed9/work/data/2cat/train\")\n",
    "nepoch = 5\n",
    "batch_size = 100\n",
    "#train_size = 500\n",
    "tr_model = None\n",
    "#i=0\n",
    "#retrain_size = 500\n",
    "#valid_size = int(math.floor(.2 * train_size))\n",
    "#print('sample size: {}'.format(train_size + valid_size))\n",
    "exp1_loss = 0.0\n",
    "exp1_accuracy = 0.0 \n",
    "exp2_loss = 0.0\n",
    "exp2_accuracy = 0.0 \n",
    "\n",
    "exp1_loss_array = []\n",
    "exp1_accuracy_array = []\n",
    "exp1_training_size = []\n",
    "exp2_loss_array = []\n",
    "exp2_accuracy_array = []\n",
    "exp2_training_size = []\n",
    "\n",
    "#build test set...\n",
    "move_random_data(2000, data_dir_F, test_path, True)\n",
    "r_set = get_data_blocks(data_dir_F)\n",
    "\n",
    "# for block in r_set:\n",
    "\n",
    "#     probs, test_batches,loss, accuracy, train_size, valid_size = run_experiment1(block, nepoch, batch_size, tr_model)\n",
    "\n",
    "#     exp1_accuracy_array.append(accuracy)\n",
    "#     exp1_loss_array.append(loss)\n",
    "#     exp1_training_size.append(train_size + valid_size)\n",
    "\n",
    "#     print(exp1_training_size)\n",
    "#     print(exp1_accuracy_array)\n",
    "#     print(exp1_loss_array)\n",
    "\n",
    "#preparations for experiment#2    \n",
    "tr_model = None\n",
    "#prep_experiment2(r_set[0], data_dir_F, 'unlabel')    \n",
    "#for block in r_set:\n",
    "probs, test_batches, loss, accuracy, train_size, valid_size = run_experiment2(r_set, nepoch, batch_size, tr_model)\n",
    "    \n",
    "\n",
    "#conduct_experiments()\n",
    "#experiment:1\n",
    "#experiment#1\n",
    "#move\n",
    "#train\n",
    "#test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
