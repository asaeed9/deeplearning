{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/asaeed9/work/deeplearning\n"
     ]
    }
   ],
   "source": [
    "%cd ~/work/deeplearning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 1: GeForce GTX 980 Ti (CNMeM is disabled, cuDNN 5110)\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from theano.sandbox import cuda\n",
    "cuda.use('gpu1')\n",
    "%matplotlib inline\n",
    "from __future__ import print_function, division\n",
    "#path=\"../data/2cat/sample\"\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "from IPython.display import FileLink\n",
    "from keras.preprocessing import image, sequence\n",
    "import os, sys, cv2\n",
    "from shutil import copyfile, move\n",
    "from random import shuffle\n",
    "\n",
    "####\n",
    "from keras.layers.convolutional import *\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/asaeed9/work/data/2cat/train\n"
     ]
    }
   ],
   "source": [
    "data_dir=\"/home/asaeed9/work/data/2cat\"\n",
    "path=\"/home/asaeed9/work/data/2cat/sample/\"\n",
    "results_path = \"/home/asaeed9/work/data/2cat/sample/results\"\n",
    "test_path = path + '/test/' #We use all the test data\n",
    "global last_file_timestamp\n",
    "%cd ../data/2cat/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adjust_prev_data_sample():\n",
    "    %mv $path/train/cats/* $data_dir/train/\n",
    "    %mv $path/train/dogs/* $data_dir/train/\n",
    "    %mv $path/valid/cats/* $data_dir/train/\n",
    "    %mv $path/valid/dogs/* $data_dir/train/\n",
    "    \n",
    "#clean previous data\n",
    "def adjust_prev_data():\n",
    "    %mv $data_dir/valid/* $data_dir/train/    \n",
    "    adjust_prev_data_sample()\n",
    "\n",
    "def move_data(images, kind, clean):\n",
    "    \n",
    "    if clean:\n",
    "        %mv $path/$kind/cats/* $data_dir/train/\n",
    "        %mv $path/$kind/dogs/* $data_dir/train/\n",
    "        \n",
    "    g = glob('*.jpg')\n",
    "    shuf = np.random.permutation(g)\n",
    "    for i in range(images): os.rename(shuf[i], '../sample/'+ kind + '/' + shuf[i]) \n",
    "    %mv ../sample/$kind/cat*.jpg ../sample/$kind/cats/\n",
    "    %mv ../sample/$kind/dog*.jpg ../sample/$kind/dogs/      \n",
    "\n",
    "# def move_to_unlabel(images):\n",
    "#     g = glob('*.jpg')\n",
    "#     shuf = np.random.permutation(g)\n",
    "#     for i in range(images): os.rename(shuf[i], '../sample/unlabel/' + shuf[i]) \n",
    "#     %mv ../sample/unlabel/cat*.jpg ../sample/unlabel/cats/\n",
    "#     %mv ../sample/unlabel/dog*.jpg ../sample/unlabel/dogs/      \n",
    "    \n",
    "    \n",
    "def handle_null(train, validation):\n",
    "    g = glob('*.jpg')\n",
    "    shuf = np.random.permutation(g)\n",
    "    for i in range(train): copyfile(shuf[i], '../sample/train/' + shuf[i]) \n",
    "    %mv ../sample/train/cat*.jpg ../sample/train/cats/\n",
    "    %mv ../sample/train/dog*.jpg ../sample/train/dogs/\n",
    "    \n",
    "    %cd ../valid\n",
    "\n",
    "    g = glob('*.jpg')\n",
    "    shuf = np.random.permutation(g)\n",
    "    for i in range(validation): copyfile(shuf[i], '../sample/valid/' + shuf[i]) \n",
    "    %mv ../sample/valid/cat*.jpg ../sample/valid/cats/\n",
    "    %mv ../sample/valid/dog*.jpg ../sample/valid/dogs/\n",
    "    %cd $data_dir/train\n",
    "    \n",
    "#copy training images\n",
    "def copy_samples(train, validation):\n",
    "    #print(\"Copying new samples for training...\")\n",
    "    #clean previous data\n",
    "    adjust_prev_data()\n",
    "    \n",
    "    #build validation set\n",
    "    g = glob('*.jpg')\n",
    "    shuf = np.random.permutation(g)\n",
    "    for i in range(validation): os.rename(shuf[i], '../valid/' + shuf[i]) \n",
    "    \n",
    "    g = glob('*.jpg')\n",
    "    shuf = np.random.permutation(g)\n",
    "    for i in range(train): copyfile(shuf[i], '../sample/train/' + shuf[i]) \n",
    "    %mv ../sample/train/cat*.jpg ../sample/train/cats/\n",
    "    %mv ../sample/train/dog*.jpg ../sample/train/dogs/\n",
    "    \n",
    "    %cd ../valid\n",
    "\n",
    "    g = glob('*.jpg')\n",
    "    shuf = np.random.permutation(g)\n",
    "    for i in range(validation): copyfile(shuf[i], '../sample/valid/' + shuf[i]) \n",
    "    %mv ../sample/valid/cat*.jpg ../sample/valid/cats/\n",
    "    %mv ../sample/valid/dog*.jpg ../sample/valid/dogs/\n",
    "    %cd $data_dir/train\n",
    "\n",
    "def onehot(x): return np.array(OneHotEncoder().fit_transform(x.reshape(-1,1)).todense())\n",
    "\n",
    "def pred_batch(imgs, classes):\n",
    "    preds = model.predict(imgs)\n",
    "    idxs = np.argmax(preds, axis=1)\n",
    "\n",
    "    print('Shape: {}'.format(preds.shape))\n",
    "    print('First 5 classes: {}'.format(classes[:5]))\n",
    "    print('First 5 probabilities: {}\\n'.format(preds[:5]))\n",
    "    print('Predictions prob/class: ')\n",
    "    \n",
    "    for i in range(len(idxs)):\n",
    "        idx = idxs[i]\n",
    "        print ('  {:.4f}/{}'.format(preds[i, idx], classes[idx]))\n",
    "\n",
    "\n",
    "def generate_size_graph(fig_no, training_size, accuracy, loss, start_size, end_size):\n",
    "    plt.figure(fig_no,figsize=(7,5))\n",
    "    plt.plot(training_size,accuracy)\n",
    "    plt.plot(training_size,loss)\n",
    "    plt.xlabel('Training Size')\n",
    "    plt.ylabel('Accuracy/Loss')\n",
    "    plt.title('Training Size vs Accuracy/Loss')\n",
    "    plt.grid(True)\n",
    "    plt.legend(['Accuracy','Loss'])\n",
    "    plt.style.use(['classic'])\n",
    "    plt.show()\n",
    "    plt.savefig(path + '/batch_graphs/' +  str(start_size) + '_' + str(end_size) + '.jpg') \n",
    "        \n",
    "def generate_graph(fig_no, epochs, train, val, label, train_title, val_title, train_size):\n",
    "    plt.figure(fig_no,figsize=(7,5))\n",
    "    plt.plot(epochs,train)\n",
    "    plt.plot(epochs,val)\n",
    "    plt.xlabel('num of Epochs')\n",
    "    plt.ylabel(label)\n",
    "    plt.title(train_title + ' vs ' + val_title + '( Samples:' + str(train_size) + ')')\n",
    "    plt.grid(True)\n",
    "    plt.legend(['train','val'])\n",
    "    plt.style.use(['classic'])\n",
    "    plt.show()\n",
    "    plt.savefig(results_path + '/batch_graphs/' +  label + '_' + str(train_size) + '.jpg') \n",
    "    \n",
    "def get_train_model(tr_batches, val_batches, epoch):\n",
    "    model = Sequential([\n",
    "            BatchNormalization(axis=1, input_shape=(3, 256,256)),\n",
    "            Convolution2D(32,3,3, activation='relu'),\n",
    "            BatchNormalization(axis=1),\n",
    "            MaxPooling2D((3,3)),\n",
    "            Dropout(0.2),\n",
    "#             Convolution2D(64,3,3, activation='relu'),\n",
    "#             BatchNormalization(axis=1),\n",
    "            #MaxPooling2D((3,3)),\n",
    "            Convolution2D(64,3,3, activation='relu'),\n",
    "            BatchNormalization(axis=1),\n",
    "            MaxPooling2D((3,3)),\n",
    "            Dropout(0.2),\n",
    "            Convolution2D(32,3,3, activation='relu'),\n",
    "            BatchNormalization(axis=1),\n",
    "            MaxPooling2D((3,3)),\n",
    "            Dropout(0.2),\n",
    "            Flatten(),\n",
    "            Dense(1024, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.2),\n",
    "            Dense(2, activation='softmax')\n",
    "        ])\n",
    "\n",
    "    model.compile(Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit_generator(tr_batches, tr_batches.nb_sample, nb_epoch=2, validation_data=val_batches, \n",
    "                     nb_val_samples=val_batches.nb_sample)\n",
    "    model.optimizer.lr = 0.001\n",
    "    model.fit_generator(tr_batches, tr_batches.nb_sample, nb_epoch=epoch - 2, validation_data=val_batches, \n",
    "                     nb_val_samples=val_batches.nb_sample)\n",
    "    return model \n",
    "\n",
    "def train_model(model, tr_batches, val_batches, epoch):\n",
    "    if not model:\n",
    "        model = Sequential([\n",
    "                BatchNormalization(axis=1, input_shape=(3, 256,256)),\n",
    "                Convolution2D(32,3,3, activation='relu'),\n",
    "                BatchNormalization(axis=1),\n",
    "                MaxPooling2D((3,3)),\n",
    "                Dropout(0.2),\n",
    "    #             Convolution2D(64,3,3, activation='relu'),\n",
    "    #             BatchNormalization(axis=1),\n",
    "                #MaxPooling2D((3,3)),\n",
    "                Convolution2D(64,3,3, activation='relu'),\n",
    "                BatchNormalization(axis=1),\n",
    "                MaxPooling2D((3,3)),\n",
    "                Dropout(0.2),\n",
    "                Convolution2D(32,3,3, activation='relu'),\n",
    "                BatchNormalization(axis=1),\n",
    "                MaxPooling2D((3,3)),\n",
    "                Dropout(0.2),\n",
    "                Flatten(),\n",
    "                Dense(1024, activation='relu'),\n",
    "                BatchNormalization(),\n",
    "                Dropout(0.2),\n",
    "                Dense(2, activation='softmax')\n",
    "            ])\n",
    "        model.compile(Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "       \n",
    "    model.fit_generator(tr_batches, tr_batches.nb_sample, nb_epoch=2, validation_data=val_batches, \n",
    "                     nb_val_samples=val_batches.nb_sample)\n",
    "    \n",
    "    model.optimizer.lr = 0.1\n",
    "    model.fit_generator(tr_batches, tr_batches.nb_sample, nb_epoch=1, validation_data=val_batches, \n",
    "                     nb_val_samples=val_batches.nb_sample)\n",
    "    \n",
    "    model.optimizer.lr = 0.001\n",
    "    model.fit_generator(tr_batches, tr_batches.nb_sample, nb_epoch=epoch - 3, validation_data=val_batches, \n",
    "                     nb_val_samples=val_batches.nb_sample)\n",
    "        \n",
    "    return model \n",
    "    \n",
    "    \n",
    "def get_test_model():\n",
    "    model = Sequential([\n",
    "                BatchNormalization(axis=1, input_shape=(3,256,256)),\n",
    "                Convolution2D(32,3,3, activation='relu'),\n",
    "                BatchNormalization(axis=1),\n",
    "                MaxPooling2D((3,3)),\n",
    "    #             Convolution2D(64,3,3, activation='relu'),\n",
    "    #             BatchNormalization(axis=1),\n",
    "                #MaxPooling2D((3,3)),\n",
    "                Convolution2D(64,3,3, activation='relu'),\n",
    "                BatchNormalization(axis=1),\n",
    "                MaxPooling2D((3,3)),\n",
    "                Convolution2D(32,3,3, activation='relu'),\n",
    "                BatchNormalization(axis=1),\n",
    "                MaxPooling2D((3,3)),\n",
    "                Flatten(),\n",
    "                Dense(1024, activation='relu'),\n",
    "                BatchNormalization(),\n",
    "                Dense(2, activation='softmax')\n",
    "            ])\n",
    "\n",
    "    model.compile(Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])  \n",
    "    \n",
    "    return model\n",
    "\n",
    "def fit(samples_copied, old_model, path, results_path, nepoch, batch_size, train_size, valid_size):\n",
    "    gen_t = image.ImageDataGenerator(rotation_range=15, height_shift_range=0.05, \n",
    "                shear_range=0.1, channel_shift_range=20, width_shift_range=0.1)\n",
    "\n",
    "    #for train in training_range:\n",
    "    model = None\n",
    "    if not samples_copied:\n",
    "        copy_samples(train_size, valid_size)\n",
    "\n",
    "    tr_batches = gen_t.flow_from_directory(path + 'train', batch_size=batch_size)\n",
    "    val_batches = gen_t.flow_from_directory(path + 'valid', class_mode='categorical', shuffle=True, batch_size=batch_size * 2)\n",
    "    \n",
    "    if old_model:\n",
    "        model = train_model(old_model, tr_batches, val_batches, nepoch)\n",
    "    else:\n",
    "        model = train_model(None, tr_batches, val_batches, nepoch)\n",
    "        \n",
    "    #model.save_weights(results_path+ '/' + 'ft_' + str(train_size) + '.e' + str(nepoch))\n",
    "    last_file_timestamp = '{:%Y%m%d%H%M%S}'.format(datetime.datetime.now())\n",
    "    #print('File Time Stamp:{}'.format(last_file_timestamp))\n",
    "    model.save_weights(results_path+'/ft_{}'.format(last_file_timestamp))\n",
    "    #model.load_weights(results_path+'/ft_{}'.format(last_file_timestamp))\n",
    "                          \n",
    "    return model, last_file_timestamp\n",
    "\n",
    "def predict(path, model, predict_type):\n",
    "    gen_test = image.ImageDataGenerator()\n",
    "    test_batches = gen_test.flow_from_directory(path+predict_type, class_mode=None, target_size=(256,256), shuffle=False, batch_size=1)\n",
    "    test_data = np.concatenate([test_batches.next() for i in range(test_batches.nb_sample)])\n",
    "    test_labels = onehot(test_batches.classes)\n",
    "    score = model.evaluate(test_data, test_labels)\n",
    "    \n",
    "    probs = model.predict(test_data)\n",
    "    \n",
    "    #loss_score.append(score[0])\n",
    "    #accuracy_score.append(score[1])\n",
    "    \n",
    "    #print(\"\\nLoss:{}, Accuracy:{}\".format(score[0], score[1]))\n",
    "    #print(\"\\nProbabilities:{}\".format(probs))\n",
    "    return probs, test_batches, score[0], score[1]\n",
    "\n",
    "def move_samples(retrain_set,dest_path, n, limit):\n",
    "    cats_copied = 0\n",
    "    dogs_copied = 0\n",
    "    retrain_list = list(retrain_set)\n",
    "    shuffle(retrain_list)\n",
    "    for fil in range(n):\n",
    "        fil = retrain_list.pop()\n",
    "        fil_cpy = fil[fil.find('/')+1:]\n",
    "\n",
    "        if 'cat' in fil_cpy and cats_copied <= limit:\n",
    "            os.rename(os.path.join(path + \"unlabel/cats/\"+ fil_cpy), os.path.join(path + dest_path + \"/cats/\"+ fil_cpy))\n",
    "            cats_copied+=1\n",
    "        elif 'dog' in fil_cpy and dogs_copied <= limit:\n",
    "            os.rename(os.path.join(path + \"unlabel/dogs/\"+ fil_cpy), os.path.join(path + dest_path + \"/dogs/\" + fil_cpy))\n",
    "            dogs_copied+=1\n",
    "            \n",
    "    #print(\"Limit:\", limit)        \n",
    "    #print(\"moved cats:\", cats_copied)\n",
    "    #print(\"moved dogs:\", dogs_copied)\n",
    "    #print(\"Retrain Length: \", len(retrain_list))\n",
    "    return retrain_list, cats_copied, dogs_copied\n",
    "\n",
    "\n",
    "def move_to_train(retrain_set, limit):\n",
    "    cats = 0\n",
    "    dogs = 0\n",
    "    valid_limit = int(math.floor(.2*(limit*2)))\n",
    "    train_limit = int(math.floor(.8*(limit*2)))\n",
    "    \n",
    "    #print(retrain_set[:10])\n",
    "    print(\"validation set: \", valid_limit)\n",
    "    retrain_list, cats_copied, dogs_copied = move_samples(retrain_set, \"valid\", valid_limit, limit)\n",
    "    valid_size = cats_copied + dogs_copied\n",
    "    print(\"Train set: \", train_limit)\n",
    "    retrain_list, cats_copied, dogs_copied = move_samples(retrain_list, \"train\", train_limit, limit)\n",
    "    train_size = cats_copied + dogs_copied\n",
    "    \n",
    "    return train_size, valid_size, train_size + valid_size\n",
    "\n",
    "def refil_unlabel(nimages):\n",
    "    os.chdir(\"/home/asaeed9/work/data/2cat/train\")\n",
    "    g = glob('*.jpg')\n",
    "    shuf = np.random.permutation(g)\n",
    "    for i in range(nimages): os.rename(shuf[i], '../sample/unlabel/' + shuf[i])\n",
    "    #os.chdir(\"../sample/test/\")\n",
    "    #move(\"cat*.jpg\")\n",
    "    %mv ../sample/unlabel/cat*.jpg ../sample/unlabel/cats/\n",
    "    %mv ../sample/unlabel/dog*.jpg ../sample/unlabel/dogs/     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_data(2000, 'test', True)\n",
    "move_data(2000, 'unlabel', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/asaeed9/work/data/2cat/train\n",
      "Train Size:100\n",
      "Valid Size:20\n",
      "/home/asaeed9/work/data/2cat/valid\n",
      "/home/asaeed9/work/data/2cat/train\n",
      "Found 100 images belonging to 2 classes.\n",
      "Found 20 images belonging to 2 classes.\n",
      "Epoch 1/2\n",
      "100/100 [==============================] - 1s - loss: 1.1483 - acc: 0.5400 - val_loss: 2.3507 - val_acc: 0.3500\n",
      "Epoch 2/2\n",
      "100/100 [==============================] - 1s - loss: 1.0920 - acc: 0.5000 - val_loss: 1.2636 - val_acc: 0.3500\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s - loss: 1.1124 - acc: 0.5200 - val_loss: 0.9794 - val_acc: 0.5000\n",
      "Epoch 1/37\n",
      "100/100 [==============================] - 1s - loss: 0.9895 - acc: 0.5200 - val_loss: 0.6053 - val_acc: 0.7000\n",
      "Epoch 2/37\n",
      "100/100 [==============================] - 1s - loss: 0.7074 - acc: 0.6200 - val_loss: 0.7069 - val_acc: 0.5500\n",
      "Epoch 3/37\n",
      "100/100 [==============================] - 1s - loss: 0.9876 - acc: 0.5700 - val_loss: 0.5560 - val_acc: 0.8000\n",
      "Epoch 4/37\n",
      "100/100 [==============================] - 1s - loss: 0.7944 - acc: 0.6200 - val_loss: 0.6713 - val_acc: 0.5500\n",
      "Epoch 5/37\n",
      "100/100 [==============================] - 1s - loss: 0.8882 - acc: 0.5800 - val_loss: 0.6550 - val_acc: 0.5500\n",
      "Epoch 6/37\n",
      "100/100 [==============================] - 1s - loss: 0.8043 - acc: 0.6700 - val_loss: 0.6090 - val_acc: 0.7000\n",
      "Epoch 7/37\n",
      "100/100 [==============================] - 1s - loss: 0.5444 - acc: 0.7400 - val_loss: 0.7097 - val_acc: 0.6500\n",
      "Epoch 8/37\n",
      "100/100 [==============================] - 1s - loss: 0.5083 - acc: 0.7900 - val_loss: 0.7422 - val_acc: 0.7000\n",
      "Epoch 9/37\n",
      "100/100 [==============================] - 1s - loss: 0.5806 - acc: 0.6900 - val_loss: 0.6502 - val_acc: 0.7000\n",
      "Epoch 10/37\n",
      "100/100 [==============================] - 1s - loss: 0.6293 - acc: 0.6700 - val_loss: 0.6769 - val_acc: 0.6500\n",
      "Epoch 11/37\n",
      "100/100 [==============================] - 1s - loss: 0.6742 - acc: 0.6400 - val_loss: 0.8278 - val_acc: 0.6500\n",
      "Epoch 12/37\n",
      "100/100 [==============================] - 1s - loss: 0.5332 - acc: 0.7700 - val_loss: 0.8534 - val_acc: 0.6500\n",
      "Epoch 13/37\n",
      "100/100 [==============================] - 1s - loss: 0.6213 - acc: 0.7700 - val_loss: 0.8483 - val_acc: 0.6500\n",
      "Epoch 14/37\n",
      "100/100 [==============================] - 1s - loss: 0.4863 - acc: 0.7800 - val_loss: 0.8964 - val_acc: 0.6500\n",
      "Epoch 15/37\n",
      "100/100 [==============================] - 1s - loss: 0.4578 - acc: 0.8000 - val_loss: 0.9823 - val_acc: 0.6500\n",
      "Epoch 16/37\n",
      "100/100 [==============================] - 1s - loss: 0.4705 - acc: 0.8100 - val_loss: 0.9795 - val_acc: 0.6500\n",
      "Epoch 17/37\n",
      "100/100 [==============================] - 1s - loss: 0.5526 - acc: 0.7300 - val_loss: 1.0207 - val_acc: 0.6500\n",
      "Epoch 18/37\n",
      "100/100 [==============================] - 1s - loss: 0.4503 - acc: 0.7800 - val_loss: 0.8813 - val_acc: 0.6500\n",
      "Epoch 19/37\n",
      "100/100 [==============================] - 1s - loss: 0.5997 - acc: 0.7400 - val_loss: 1.0396 - val_acc: 0.6500\n",
      "Epoch 20/37\n",
      "100/100 [==============================] - 1s - loss: 0.3701 - acc: 0.8500 - val_loss: 0.9977 - val_acc: 0.6500\n",
      "Epoch 21/37\n",
      "100/100 [==============================] - 1s - loss: 0.3354 - acc: 0.8600 - val_loss: 1.0918 - val_acc: 0.6500\n",
      "Epoch 22/37\n",
      "100/100 [==============================] - 1s - loss: 0.3613 - acc: 0.8200 - val_loss: 1.0659 - val_acc: 0.6500\n",
      "Epoch 23/37\n",
      "100/100 [==============================] - 1s - loss: 0.4406 - acc: 0.8200 - val_loss: 1.0851 - val_acc: 0.6500\n",
      "Epoch 24/37\n",
      "100/100 [==============================] - 1s - loss: 0.5446 - acc: 0.7600 - val_loss: 1.0032 - val_acc: 0.6500\n",
      "Epoch 25/37\n",
      "100/100 [==============================] - 1s - loss: 0.4059 - acc: 0.8000 - val_loss: 1.2016 - val_acc: 0.6500\n",
      "Epoch 26/37\n",
      "100/100 [==============================] - 1s - loss: 0.4110 - acc: 0.7900 - val_loss: 1.1626 - val_acc: 0.6500\n",
      "Epoch 27/37\n",
      "100/100 [==============================] - 1s - loss: 0.3594 - acc: 0.8500 - val_loss: 1.1880 - val_acc: 0.6500\n",
      "Epoch 28/37\n",
      "100/100 [==============================] - 1s - loss: 0.4292 - acc: 0.8200 - val_loss: 1.2525 - val_acc: 0.6500\n",
      "Epoch 29/37\n",
      "100/100 [==============================] - 1s - loss: 0.3030 - acc: 0.8800 - val_loss: 1.2088 - val_acc: 0.6500\n",
      "Epoch 30/37\n",
      "100/100 [==============================] - 1s - loss: 0.3485 - acc: 0.8700 - val_loss: 1.4281 - val_acc: 0.6500\n",
      "Epoch 31/37\n",
      "100/100 [==============================] - 1s - loss: 0.3406 - acc: 0.8500 - val_loss: 1.3192 - val_acc: 0.6500\n",
      "Epoch 32/37\n",
      "100/100 [==============================] - 1s - loss: 0.4071 - acc: 0.8200 - val_loss: 1.3975 - val_acc: 0.6500\n",
      "Epoch 33/37\n",
      "100/100 [==============================] - 1s - loss: 0.4118 - acc: 0.8500 - val_loss: 1.4127 - val_acc: 0.6500\n",
      "Epoch 34/37\n",
      "100/100 [==============================] - 1s - loss: 0.4186 - acc: 0.8000 - val_loss: 1.4576 - val_acc: 0.6500\n",
      "Epoch 35/37\n",
      "100/100 [==============================] - 1s - loss: 0.3484 - acc: 0.8300 - val_loss: 1.4469 - val_acc: 0.6500\n",
      "Epoch 36/37\n",
      "100/100 [==============================] - 1s - loss: 0.3049 - acc: 0.8900 - val_loss: 1.4889 - val_acc: 0.6500\n",
      "Epoch 37/37\n",
      "100/100 [==============================] - 1s - loss: 0.3141 - acc: 0.8200 - val_loss: 1.5563 - val_acc: 0.6500\n",
      "\n",
      "Verification on Unlabel set.\n",
      "Found 2000 images belonging to 2 classes.\n",
      "2000/2000 [==============================] - 2s     \n",
      "\n",
      "Unlabel Accuracy:0.49\n",
      "Unlabel Loss:2.13622961034\n",
      "validation set:  343\n",
      "Train set:  1374\n",
      "\n",
      "Verification on Test set.\n",
      "Found 2000 images belonging to 2 classes.\n",
      "2000/2000 [==============================] - 2s     \n",
      "\n",
      "Training Set Size:[1474]\n",
      "Accuracy:[0.4945]\n",
      "Loss:[2.1143913902044296]\n",
      "Train Size:1374\n",
      "Valid Size:343\n",
      "Found 1474 images belonging to 2 classes.\n",
      "Found 363 images belonging to 2 classes.\n",
      "Epoch 1/2\n",
      "1474/1474 [==============================] - 25s - loss: 1.0937 - acc: 0.5767 - val_loss: 2.2450 - val_acc: 0.5041\n",
      "Epoch 2/2\n",
      "1474/1474 [==============================] - 18s - loss: 0.9107 - acc: 0.5875 - val_loss: 2.2007 - val_acc: 0.5014\n",
      "Epoch 1/1\n",
      "1474/1474 [==============================] - 24s - loss: 0.8334 - acc: 0.6262 - val_loss: 2.2321 - val_acc: 0.4876\n",
      "Epoch 1/37\n",
      "1474/1474 [==============================] - 24s - loss: 0.8281 - acc: 0.6106 - val_loss: 1.9648 - val_acc: 0.5041\n",
      "Epoch 2/37\n",
      "1474/1474 [==============================] - 18s - loss: 0.7659 - acc: 0.6357 - val_loss: 1.8135 - val_acc: 0.5014\n",
      "Epoch 3/37\n",
      "1474/1474 [==============================] - 18s - loss: 0.7390 - acc: 0.6459 - val_loss: 1.8378 - val_acc: 0.4766\n",
      "Epoch 4/37\n",
      "1474/1474 [==============================] - 18s - loss: 0.7672 - acc: 0.6235 - val_loss: 1.5705 - val_acc: 0.5041\n",
      "Epoch 5/37\n",
      "1474/1474 [==============================] - 18s - loss: 0.7337 - acc: 0.6350 - val_loss: 1.5629 - val_acc: 0.4959\n",
      "Epoch 6/37\n",
      "1410/1474 [===========================>..] - ETA: 0s - loss: 0.7528 - acc: 0.6355"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-96b0ebc6bbaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mvalid_size\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mtr_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfile_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-fcad7386af0b>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(samples_copied, old_model, path, results_path, nepoch, batch_size, train_size, valid_size)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mold_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-fcad7386af0b>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, tr_batches, val_batches, epoch)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     model.fit_generator(tr_batches, tr_batches.nb_sample, nb_epoch=epoch - 3, validation_data=val_batches, \n\u001b[0;32m--> 185\u001b[0;31m                      nb_val_samples=val_batches.nb_sample)\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asaeed9/anaconda2/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    933\u001b[0m                                         \u001b[0mnb_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_worker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m                                         \u001b[0mpickle_safe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_safe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m     def evaluate_generator(self, generator, val_samples,\n",
      "\u001b[0;32m/home/asaeed9/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1583\u001b[0m                                 \u001b[0mmax_q_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_q_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m                                 \u001b[0mnb_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_worker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1585\u001b[0;31m                                 pickle_safe=pickle_safe)\n\u001b[0m\u001b[1;32m   1586\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1587\u001b[0m                             \u001b[0;31m# no need for try/except because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asaeed9/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(self, generator, val_samples, max_q_size, nb_worker, pickle_safe)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m                         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%cd $data_dir/train\n",
    "\n",
    "nepoch = 40\n",
    "batch_size = 64\n",
    "train_size = 100\n",
    "running_train_size = 100\n",
    "retrain_size = 1800\n",
    "training_set_size = []\n",
    "valid_size = int(math.floor(.2 * train_size))\n",
    "#print('sample size: {}'.format(train_size + valid_size))\n",
    "tr_model = None\n",
    "loss = 0.0\n",
    "loss_array = []\n",
    "accuracy = 0.0\n",
    "accuracy_array = []\n",
    "\n",
    "#copy test data\n",
    "#move_data(2000, 'test', True)\n",
    "#move_data(2000, 'unlabel', True)\n",
    "\n",
    "i=0\n",
    "for i in range(200):\n",
    "    print(\"Train Size:{}\".format(train_size))\n",
    "    print(\"Valid Size:{}\".format(valid_size))\n",
    "\n",
    "    if train_size == 0 or valid_size == 0: #handle null case\n",
    "        handle_null(50,10)\n",
    "        train_size += 50\n",
    "        valid_size += 10\n",
    "\n",
    "    tr_model,file_timestamp = fit(i, tr_model, path, results_path, nepoch, batch_size, train_size, valid_size)\n",
    "\n",
    "    model = None\n",
    "    model = get_test_model() \n",
    "    #model.load_weights(results_path+'/ft_' + str(train_size) + '.e' + str(nepoch))\n",
    "    #print('{0}/ft_{1}'.format(last_file_timestamp))\n",
    "    #print('Last File Timestamp- before loading:{}'.format(file_timestamp))\n",
    "    model.load_weights(results_path+'/ft_{}'.format(file_timestamp))\n",
    "\n",
    "    print(\"\\nVerification on Unlabel set.\")\n",
    "    probs, test_batches,loss, accuracy = predict(path, model, \"unlabel\")\n",
    "    print('\\nUnlabel Accuracy:{}'.format(accuracy))\n",
    "    print('Unlabel Loss:{}'.format(loss))    \n",
    "#     training_set_size.append(running_train_size)\n",
    "#     loss_array.append(loss)\n",
    "#     accuracy_array.append(accuracy)\n",
    "\n",
    "    #get the top 100, most confused images\n",
    "    retrain_idx = np.argsort(probs[:,0] - probs[:, 1])[:retrain_size]\n",
    "    #print(len(retrain_idx))\n",
    "\n",
    "    retrain_set = [test_batches.filenames[i] for i in retrain_idx]\n",
    "\n",
    "    #print('Retrain Set Length:{}'.format(len(retrain_set)))\n",
    "\n",
    "    os.chdir(path + 'unlabel')\n",
    "    ndog = sum('dog' in name for name in retrain_set)\n",
    "    ncat =  sum('cat' in name for name in retrain_set)\n",
    "    limit = min(ncat, ndog)\n",
    "\n",
    "    #print('Dogs:{}, Cats:{}'.format(ndog, ncat))\n",
    "\n",
    "    #move existing training data to the store\n",
    "    #adjust_prev_data_sample()\n",
    "    train_size, valid_size, copied_images = move_to_train(retrain_set, limit)\n",
    "    refil_unlabel(copied_images)\n",
    "    \n",
    "\n",
    "    print(\"\\nVerification on Test set.\")\n",
    "    model_test = None\n",
    "    model_test = get_test_model() \n",
    "    #model.load_weights(results_path+'/ft_' + str(train_size) + '.e' + str(nepoch))\n",
    "    #print('{0}/ft_{1}'.format(last_file_timestamp))\n",
    "    #print('Last File Timestamp- before loading:{}'.format(file_timestamp))\n",
    "    model_test.load_weights(results_path+'/ft_{}'.format(file_timestamp))\n",
    "\n",
    "    probs, test_batches,loss, accuracy = predict(path, model_test, \"test\")\n",
    "    training_set_size.append(running_train_size)\n",
    "    loss_array.append(loss)\n",
    "    accuracy_array.append(accuracy)\n",
    "\n",
    "    running_train_size += train_size\n",
    "\n",
    "    print('\\nTraining Set Size:{}'.format(training_set_size))\n",
    "    print('Accuracy:{}'.format(accuracy_array))\n",
    "    print('Loss:{}'.format(loss_array))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_probs = probs[:3, :]\n",
    "\n",
    "np.argsort(sample_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_probs[:3,0] - sample_probs[:3, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.argsort(sample_probs[:3,0] - sample_probs[:3, 1])[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted(probs[:2,0] - probs[:2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "np.where(sorted(abs(probs[:2,0] - probs[:2,1])))[0]\n",
    "\n",
    "#np.where(np.logical_and(probs >=0.4, probs<=0.7))[0]\n",
    "#np.where()[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.where(abs(probs[:,0] - probs[:, 1]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Training Set Size:[100, 255, 507, 544, 586, 669, 727, 786, 829, 1097, 1166, 1288, 1322, 1610, 1795, 1979, 2124, 2384, 2555, 2707, 2903, 3015, 3078, 3151, 3306, 3424, 3480, 3544, 3614, 3762, 3984, 4203, 4355, 4502, 4638, 4758, 4830, 4918, 4974, 5002, 5014, 5049, 5083, 5108, 5121, 5163, 5196, 5271, 5297, 5338, 5377, 5442, 5478, 5585, 5647, 5744, 5804, 5842, 5885, 5926, 5964, 5990, 5996, 6050, 6098, 6114, 6137, 6156, 6292, 6360, 6493, 6603, 6641, 6672, 6705, 6740, 6856, 6885, 6969, 7022, 7057, 7103, 7119, 7137, 7157, 7167, 7170, 7192, 7259, 7279, 7326, 7368, 7409, 7489, 7508, 7519, 7535, 7565, 7577, 7593]\n",
    "Accuracy:[0.51834556370913942, 0.53235490327878665, 0.54069379586390931, 0.55103402269172863, 0.56370913944616563, 0.56170780524323194, 0.57171447635730799, 0.57738492332195102, 0.59472981993956431, 0.607738492358041, 0.61674449629112427, 0.62408272182448377, 0.65310206809506721, 0.6711140761103449, 0.67645096735130394, 0.69246164113382569, 0.68478985995630803, 0.70180120085023734, 0.71247498336197779, 0.72214809877225128, 0.70713809210113721, 0.71180787194443196, 0.72014676454943605, 0.72481654442255306, 0.71581054040000314, 0.72581721151407919, 0.72314876587371812, 0.72848565714449942, 0.73048699128778871, 0.73982655113342999, 0.74583055378199414, 0.76951300877185569, 0.77451634432889371, 0.77718478995931395, 0.76484322889873868, 0.77818545695143271, 0.78118745826577407, 0.77318212137451325, 0.76751167440987011, 0.77151434285550058, 0.77384923278211837, 0.77985323556985275, 0.77451634418972337, 0.78452301542308822, 0.77585056700493349, 0.77384923278211837, 0.7795196797467614, 0.78685790532982458, 0.77985323545056395, 0.76984656442595456, 0.78519012681081191, 0.78952635096024482, 0.79319546360266535, 0.80453635753195152, 0.80553702464335919, 0.7871914609541012, 0.79719813204829582, 0.80020013338251861, 0.79786524345590082, 0.79619746493688814, 0.78819212804562733, 0.79452968641787547, 0.79853235486350593, 0.79919946639039974, 0.80086724479012361, 0.79886591056730849, 0.79986657767871605, 0.71881254179387033, 0.79686457634449326, 0.77484989991340736, 0.8115410273316862, 0.81387591735771136, 0.81087391602348857, 0.81187458303548876, 0.81120747174717256, 0.80820547029366085, 0.81654436288872423, 0.8042028018480305, 0.81621080728432904, 0.82154769854516962, 0.8232154769448935, 0.82188125412968338, 0.81420947306151392, 0.81420947306151392, 0.79786524357518962, 0.79919946627111094, 0.81854569721094683, 0.81387591735771136, 0.81454302866590911, 0.79953302199479503, 0.81320880596998768, 0.81521014007351411, 0.82088058715744605, 0.81154102731180477, 0.81120747172729102, 0.81420947296210655, 0.81854569711153946, 0.81621080728432904, 0.81721147427644791, 0.81187458301560733]\n",
    "Loss:[0.94627340202573307, 1.069129374521903, 1.6582751717068021, 1.9458960867508639, 1.7672291246949234, 1.8073495392544099, 2.2642813132276847, 2.1801735833585463, 1.4026525458786947, 1.9107522724806427, 2.0852411977643568, 1.885862276673794, 1.4222142591804088, 1.6198320426966366, 1.4607558314246762, 1.637531633791405, 1.6699402961753225, 1.4755695811345468, 1.4915868397193244, 1.3634724204066277, 1.7743482206655472, 1.8913224160949575, 1.6106715754042313, 1.3743048965970701, 1.565028794591947, 1.9099577603696425, 1.9002086268719274, 1.8303540087018593, 1.4808997903727468, 1.0050827415884933, 1.0144933128929521, 1.0415886528218405, 1.0145420589750811, 1.1418929995895626, 1.1928224458226846, 1.4710475567024974, 1.341952260054295, 1.8395282249944938, 2.3651828057511031, 2.3124792167279753, 2.305378243448915, 2.0921286372747461, 2.2739992053878604, 2.3005914534378444, 2.2752316740566392, 2.1230344122071925, 1.8697313540941323, 1.9767550084891519, 2.3808871660582858, 2.0304581639465638, 1.6816643132226141, 1.7091403857570606, 1.255017541860882, 1.4477932849790829, 1.3391912613831973, 1.695395214437166, 1.6405188316495478, 1.883696767807832, 1.8168773716272137, 2.1832812890025841, 2.5262706354183559, 2.199554436640943, 1.8084424575585736, 1.9852745032712713, 1.9270797232017987, 1.9817351747420384, 2.3064219975595739, 1.7742115364939948, 1.6118588206358875, 1.3782198212002021, 1.2986096903791102, 1.6194246127093093, 1.8720587353657601, 1.7112238509897353, 1.764265245251641, 1.3266987558938885, 1.6663410443420572, 1.6504295845829779, 1.7126746936698132, 1.9551829461497892, 1.7929846915162309, 2.0409724012221035, 2.1942079734076447, 2.3111469137771712, 2.2224564307727781, 2.5554216508963821, 2.0883331067482378, 1.6952288752798241, 2.1873511853298768, 2.0337138844416631, 1.9773875592814496, 1.9745328070035451, 1.6287168510223404, 2.3116104998768714, 2.2300192884155075, 2.162166691486481, 2.0174554670189169, 2.2769914772579942, 2.1841097289998905, 2.3013370820185077]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generate_size_graph(1, training_set_size, accuracy_array, loss_array, min(training_set_size), max(training_set_size))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
