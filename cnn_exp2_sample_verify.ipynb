{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, cv2\n",
    "os.chdir(\"/home/asaeed9/work/deeplearning\")\n",
    "from __future__ import print_function, division\n",
    "from theano.sandbox import cuda\n",
    "#cuda.use('gpu1')\n",
    "#path=\"../data/2cat/sample\"\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "from IPython.display import FileLink\n",
    "from keras.preprocessing import image, sequence\n",
    "from shutil import copyfile, move\n",
    "from random import shuffle\n",
    "\n",
    "####\n",
    "from keras.layers.convolutional import *\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir=\"/home/asaeed9/work/data/2cat\"\n",
    "data_src=\"sample\"\n",
    "path=\"/home/asaeed9/work/data/2cat/\" + data_src + \"/\"\n",
    "results_path = \"/home/asaeed9/work/data/2cat/\"+ data_src + \"/results\"\n",
    "test_path = path + '/test/' #We use all the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_file_count(dir_path):\n",
    "    return len([name for name in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path,name))])\n",
    "\n",
    "def get_train_valid_size():\n",
    "        tr_count = get_file_count(path + \"/train/cats\") + get_file_count(path + \"/train/dogs\")\n",
    "        vl_count = get_file_count(path + \"/valid/cats\") + get_file_count(path + \"/valid/dogs\")\n",
    "        \n",
    "        return tr_count, vl_count \n",
    "\n",
    "def move_data(images, kind, clean):\n",
    "    os.chdir(\"/home/asaeed9/work/data/2cat/train\")\n",
    "    n = int(images/2)\n",
    "    if clean:\n",
    "        move_files(path + kind + \"/cats\" ,data_dir + \"/\" +\"train\",\"*\")\n",
    "        move_files(path + kind + \"/dogs\" ,data_dir + \"/\" +\"train\",\"*\")\n",
    "\n",
    "    g = glob('dog*.jpg')\n",
    "    shuf = np.random.permutation(g)\n",
    "    for i in range(n): os.rename(shuf[i], \"../\" + data_src + \"/\"+ kind + '/dogs/' + shuf[i])\n",
    "\n",
    "    g = glob('cat*.jpg')\n",
    "    shuf = np.random.permutation(g)\n",
    "    for i in range(n): os.rename(shuf[i], \"../\" + data_src + \"/\"+ kind + '/cats/' + shuf[i])\n",
    "        \n",
    "\n",
    "def handle_null(train, validation):\n",
    "    g = glob('*.jpg')\n",
    "    shuf = np.random.permutation(g)\n",
    "    for i in range(train): copyfile(shuf[i], \"../\" + data_src + \"/train/\" + shuf[i])\n",
    "\n",
    "    move_files(path + \"train\",path + \"train/cats/\",\"cat*.jpg\")\n",
    "    move_files(path + \"train\",path + \"train/dogs/\",\"dog*.jpg\")\n",
    "\n",
    "    # %mv ../sample/train/cat*.jpg ../sample/train/cats/\n",
    "    # %mv ../sample/train/dog*.jpg ../sample/train/dogs/\n",
    "\n",
    "    os.chdir(\"../valid\")\n",
    "    # %cd ../valid\n",
    "\n",
    "    g = glob('*.jpg')\n",
    "    shuf = np.random.permutation(g)\n",
    "    for i in range(validation): copyfile(shuf[i], \"../\" + data_src + \"/valid/\" + shuf[i])\n",
    "\n",
    "    move_files(path + \"valid\",path + \"valid/cats/\",\"cat*.jpg\")\n",
    "    move_files(path + \"valid\",path + \"valid/dogs/\",\"dog*.jpg\")\n",
    "    os.chdir(data_dir + \"/train\")\n",
    "\n",
    "    # %mv ../sample/valid/cat*.jpg ../sample/valid/cats/\n",
    "    # %mv ../sample/valid/dog*.jpg ../sample/valid/dogs/\n",
    "    # %cd $data_dir/train    \n",
    "\n",
    "def adjust_prev_data_sample(dest):\n",
    "    move_files(path + \"train/cats\", data_dir + \"/\" + dest, \"*\")\n",
    "    move_files(path + \"train/dogs\", data_dir + \"/\" + dest, \"*\")\n",
    "    move_files(path + \"valid/cats\", data_dir + \"/\" + dest, \"*\")\n",
    "    move_files(path + \"valid/dogs\", data_dir + \"/\" + dest, \"*\")\n",
    "    \n",
    "#clean previous data\n",
    "def adjust_prev_data(dest):\n",
    "    move_files(data_dir + \"/valid\", data_dir + \"/\" + dest, \"*\")\n",
    "    move_files(data_dir + \"/used_train\", data_dir + \"/\" + dest, \"*\")\n",
    "    # %mv $data_dir/valid/* $data_dir/train/\n",
    "    adjust_prev_data_sample(dest)\n",
    "\n",
    "def copy_samples(train, validation):\n",
    "    #print(\"Copying new samples for training...\")\n",
    "\n",
    "#     build validation set\n",
    "#     g = glob('*.jpg')\n",
    "#     shuf = np.random.permutation(g)\n",
    "#     for i in range(validation): os.rename(shuf[i], '../valid/' + shuf[i])\n",
    "\n",
    "    g = glob('*.jpg')\n",
    "    shuf = np.random.permutation(g)\n",
    "    for i in range(train): os.rename(shuf[i], \"../\" + data_src + \"/train/\" + shuf[i])\n",
    "    move_files(path + \"train\",path + \"train/cats/\",\"cat*.jpg\")\n",
    "    move_files(path + \"train\",path + \"train/dogs/\",\"dog*.jpg\")\n",
    "\n",
    "    # %mv ../sample/train/cat*.jpg ../sample/train/cats/\n",
    "    # %mv ../sample/train/dog*.jpg ../sample/train/dogs/\n",
    "#     os.chdir(\"../valid\")\n",
    "    #%cd ../valid\n",
    "\n",
    "    g = glob('*.jpg')\n",
    "    shuf = np.random.permutation(g)\n",
    "    for i in range(validation): os.rename(shuf[i], \"../\" + data_src + \"/valid/\" + shuf[i])\n",
    "    move_files(path + \"valid\",path + \"valid/cats/\",\"cat*.jpg\")\n",
    "    move_files(path + \"valid\",path + \"valid/dogs/\",\"dog*.jpg\")\n",
    "    os.chdir(data_dir + \"/train\")\n",
    "\n",
    "    dog_train_count = get_file_count(path + \"train/dogs/\") \n",
    "    dog_valid_count = get_file_count(path + \"valid/dogs/\")\n",
    "\n",
    "    cat_train_count = get_file_count(path + \"train/cats/\") \n",
    "    cat_valid_count = get_file_count(path + \"valid/cats/\")\n",
    "    \n",
    "    return dog_train_count, dog_valid_count, cat_train_count, cat_valid_count\n",
    "\n",
    "def onehot(x): return np.array(OneHotEncoder().fit_transform(x.reshape(-1,1)).todense())\n",
    "\n",
    "def pred_batch(imgs, classes):\n",
    "    preds = model.predict(imgs)\n",
    "    idxs = np.argmax(preds, axis=1)\n",
    "\n",
    "    print('Shape: {}'.format(preds.shape))\n",
    "    print('First 5 classes: {}'.format(classes[:5]))\n",
    "    print('First 5 probabilities: {}\\n'.format(preds[:5]))\n",
    "    print('Predictions prob/class: ')\n",
    "\n",
    "    for i in range(len(idxs)):\n",
    "        idx = idxs[i]\n",
    "        print ('  {:.4f}/{}'.format(preds[i, idx], classes[idx]))\n",
    "\n",
    "def generate_size_graph(fig_no, training_size, accuracy, loss, start_size, end_size):\n",
    "    plt.figure(fig_no,figsize=(7,5))\n",
    "    plt.plot(training_size,accuracy)\n",
    "    plt.plot(training_size,loss)\n",
    "    plt.xlabel('Training Size')\n",
    "    plt.ylabel('Accuracy/Loss')\n",
    "    plt.title('Training Size vs Accuracy/Loss')\n",
    "    plt.grid(True)\n",
    "    plt.legend(['Accuracy','Loss'])\n",
    "    plt.style.use(['classic'])\n",
    "    plt.show()\n",
    "    plt.savefig(path + '/batch_graphs/' +  str(start_size) + '_' + str(end_size) + '.jpg')\n",
    "\n",
    "def generate_graph(fig_no, epochs, train, val, label, train_title, val_title, train_size):\n",
    "    plt.figure(fig_no,figsize=(7,5))\n",
    "    plt.plot(epochs,train)\n",
    "    plt.plot(epochs,val)\n",
    "    plt.xlabel('num of Epochs')\n",
    "    plt.ylabel(label)\n",
    "    plt.title(train_title + ' vs ' + val_title + '( Samples:' + str(train_size) + ')')\n",
    "    plt.grid(True)\n",
    "    plt.legend(['train','val'])\n",
    "    plt.style.use(['classic'])\n",
    "    plt.show()\n",
    "    plt.savefig(results_path + '/batch_graphs/' +  label + '_' + str(train_size) + '.jpg')\n",
    "\n",
    "    \n",
    "def get_train_model():\n",
    "        model = Sequential([\n",
    "                BatchNormalization(axis=1, input_shape=(3, 256,256)),\n",
    "                Convolution2D(32,3,3, activation='relu'),\n",
    "                BatchNormalization(axis=1),\n",
    "                MaxPooling2D((3,3)),\n",
    "                Dropout(0.2),\n",
    "    #             Convolution2D(64,3,3, activation='relu'),\n",
    "    #             BatchNormalization(axis=1),\n",
    "                #MaxPooling2D((3,3)),\n",
    "                Convolution2D(64,3,3, activation='relu'),\n",
    "                BatchNormalization(axis=1),\n",
    "                MaxPooling2D((3,3)),\n",
    "                Dropout(0.2),\n",
    "                Convolution2D(32,3,3, activation='relu'),\n",
    "                BatchNormalization(axis=1),\n",
    "                MaxPooling2D((3,3)),\n",
    "                Dropout(0.2),\n",
    "                Flatten(),\n",
    "                Dense(1024, activation='relu'),\n",
    "                BatchNormalization(),\n",
    "                Dropout(0.2),\n",
    "                Dense(2, activation='softmax')\n",
    "            ])\n",
    "        model.compile(Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        return model\n",
    "\n",
    "def train_model(model, tr_batches, val_batches, epoch):\n",
    "    if not model:\n",
    "        model = Sequential([\n",
    "                BatchNormalization(axis=1, input_shape=(3, 256,256)),\n",
    "                Convolution2D(32,3,3, activation='relu'),\n",
    "                BatchNormalization(axis=1),\n",
    "                MaxPooling2D((3,3)),\n",
    "                Dropout(0.2),\n",
    "    #             Convolution2D(64,3,3, activation='relu'),\n",
    "    #             BatchNormalization(axis=1),\n",
    "                #MaxPooling2D((3,3)),\n",
    "                Convolution2D(64,3,3, activation='relu'),\n",
    "                BatchNormalization(axis=1),\n",
    "                MaxPooling2D((3,3)),\n",
    "                Dropout(0.2),\n",
    "                Convolution2D(32,3,3, activation='relu'),\n",
    "                BatchNormalization(axis=1),\n",
    "                MaxPooling2D((3,3)),\n",
    "                Dropout(0.2),\n",
    "                Flatten(),\n",
    "                Dense(1024, activation='relu'),\n",
    "                BatchNormalization(),\n",
    "                Dropout(0.2),\n",
    "                Dense(2, activation='softmax')\n",
    "            ])\n",
    "        model.compile(Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.fit_generator(tr_batches, tr_batches.nb_sample, nb_epoch=2, validation_data=val_batches,\n",
    "                     nb_val_samples=val_batches.nb_sample)\n",
    "\n",
    "    model.optimizer.lr = 0.1\n",
    "    model.fit_generator(tr_batches, tr_batches.nb_sample, nb_epoch=1, validation_data=val_batches,\n",
    "                     nb_val_samples=val_batches.nb_sample)\n",
    "\n",
    "    model.optimizer.lr = 0.001\n",
    "    model.fit_generator(tr_batches, tr_batches.nb_sample, nb_epoch=epoch - 3, validation_data=val_batches,\n",
    "                     nb_val_samples=val_batches.nb_sample)\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_test_model():\n",
    "    model = Sequential([\n",
    "                BatchNormalization(axis=1, input_shape=(3,256,256)),\n",
    "                Convolution2D(32,3,3, activation='relu'),\n",
    "                BatchNormalization(axis=1),\n",
    "                MaxPooling2D((3,3)),\n",
    "    #             Convolution2D(64,3,3, activation='relu'),\n",
    "    #             BatchNormalization(axis=1),\n",
    "                #MaxPooling2D((3,3)),\n",
    "                Convolution2D(64,3,3, activation='relu'),\n",
    "                BatchNormalization(axis=1),\n",
    "                MaxPooling2D((3,3)),\n",
    "                Convolution2D(32,3,3, activation='relu'),\n",
    "                BatchNormalization(axis=1),\n",
    "                MaxPooling2D((3,3)),\n",
    "                Flatten(),\n",
    "                Dense(1024, activation='relu'),\n",
    "                BatchNormalization(),\n",
    "                Dense(2, activation='softmax')\n",
    "            ])\n",
    "\n",
    "    model.compile(Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def fit(old_model, path, results_path, nepoch, batch_size, train_size, valid_size):\n",
    "    gen_t = image.ImageDataGenerator(rotation_range=15, height_shift_range=0.05,\n",
    "                shear_range=0.1, channel_shift_range=20, width_shift_range=0.1)\n",
    "\n",
    "    tr_batches = gen_t.flow_from_directory(path + 'train', batch_size=batch_size)\n",
    "    val_batches = gen_t.flow_from_directory(path + 'valid', class_mode='categorical', \n",
    "                                            shuffle=True, batch_size=batch_size * 2)\n",
    "    model = None\n",
    "    if old_model:\n",
    "        model = train_model(old_model, tr_batches, val_batches, nepoch)\n",
    "    else:\n",
    "        model = train_model(None, tr_batches, val_batches, nepoch)\n",
    "\n",
    "    #model.summary()\n",
    "    \n",
    "    #model.save_weights(results_path+ '/' + 'ft_' + str(train_size) + '.e' + str(nepoch))\n",
    "    last_file_timestamp = '{:%Y%m%d%H%M%S}'.format(datetime.datetime.now())\n",
    "    #print('File Time Stamp:{}'.format(last_file_timestamp))\n",
    "    model.save_weights(results_path+'/ft_{}'.format(last_file_timestamp))\n",
    "    #model.load_weights(results_path+'/ft_{}'.format(last_file_timestamp))\n",
    "\n",
    "    return model, last_file_timestamp\n",
    "\n",
    "def predict(path, model, predict_type):\n",
    "    gen_test = image.ImageDataGenerator()\n",
    "    test_batches = gen_test.flow_from_directory(path+predict_type, class_mode=None, target_size=(256,256), shuffle=False, batch_size=1)\n",
    "    test_data = np.concatenate([test_batches.next() for i in range(test_batches.nb_sample)])\n",
    "    test_labels = onehot(test_batches.classes)\n",
    "    score = model.evaluate(test_data, test_labels)\n",
    "\n",
    "    probs = model.predict(test_data)\n",
    "\n",
    "    #loss_score.append(score[0])\n",
    "    #accuracy_score.append(score[1])\n",
    "\n",
    "    #print(\"\\nLoss:{}, Accuracy:{}\".format(score[0], score[1]))\n",
    "    #print(\"\\nProbabilities:{}\".format(probs))\n",
    "    return probs, test_batches, score[0], score[1]\n",
    "\n",
    "def move_samples_imbalance(retrain_set, dest_path):\n",
    "    for fil in retrain_set:\n",
    "        fil_cpy = fil[fil.find('/')+1:]  \n",
    "        \n",
    "        if \"cat\" in fil_cpy:\n",
    "            os.rename(os.path.join(path + \"unlabel/cats/\"+ fil_cpy), os.path.join(path + dest_path + \"/cats/\"+ fil_cpy))\n",
    "        else:\n",
    "            os.rename(os.path.join(path + \"unlabel/dogs/\"+ fil_cpy), os.path.join(path + dest_path + \"/dogs/\" + fil_cpy))\n",
    "\n",
    "    return len(retrain_set) #images copied   \n",
    "        \n",
    "def move_samples(retrain_set,dest_path, n, limit):\n",
    "    cats_copied = 0\n",
    "    dogs_copied = 0\n",
    "    retrain_list = list(retrain_set)\n",
    "    shuffle(retrain_list)\n",
    "    for fil in range(n):\n",
    "        fil = retrain_list.pop()\n",
    "        fil_cpy = fil[fil.find('/')+1:]\n",
    "\n",
    "        if 'cat' in fil_cpy and cats_copied <= limit:\n",
    "            os.rename(os.path.join(path + \"unlabel/cats/\"+ fil_cpy), os.path.join(path + dest_path + \"/cats/\"+ fil_cpy))\n",
    "            cats_copied+=1\n",
    "        elif 'dog' in fil_cpy and dogs_copied <= limit:\n",
    "            os.rename(os.path.join(path + \"unlabel/dogs/\"+ fil_cpy), os.path.join(path + dest_path + \"/dogs/\" + fil_cpy))\n",
    "            dogs_copied+=1\n",
    "\n",
    "    #print(\"Limit:\", limit)\n",
    "    #print(\"moved cats:\", cats_copied)\n",
    "    #print(\"moved dogs:\", dogs_copied)\n",
    "    #print(\"Retrain Length: \", len(retrain_list))\n",
    "    return retrain_list, cats_copied, dogs_copied\n",
    "\n",
    "def move_to_train(retrain_set):\n",
    "    cats = 0\n",
    "    dogs = 0    \n",
    "\n",
    "    ndog = sum('dog' in name for name in retrain_set)\n",
    "    ncat =  sum('cat' in name for name in retrain_set)\n",
    "\n",
    "    valid_dog = int(math.floor(.2*ndog))\n",
    "    valid_cat = int(math.floor(.2*ncat))\n",
    "\n",
    "    #print(retrain_set[:10])\n",
    "    print(\"validation set: \", valid_limit)\n",
    "    retrain_list, cats_copied, dogs_copied = move_samples(retrain_set, \"valid\", valid_limit, limit)\n",
    "    valid_size = cats_copied + dogs_copied\n",
    "    print(\"Train set: \", train_limit)\n",
    "    retrain_list, cats_copied, dogs_copied = move_samples(retrain_list, \"train\", train_limit, limit)\n",
    "    train_size = cats_copied + dogs_copied\n",
    "\n",
    "    return train_size, valid_size, train_size + valid_size\n",
    "\n",
    "def move_files(src_path, dest_path, pattern):\n",
    "    for file in glob(src_path + '/' + pattern):\n",
    "        try:\n",
    "                move(os.path.join(src_path+'/', os.path.basename(file)), os.path.join(dest_path+'/',os.path.basename(file)))\n",
    "        except IOError, e:\n",
    "                print (\"Unable to move file. \".format(e))\n",
    "\n",
    "def refil_unlabel(ndogs, ncats, segment):\n",
    "    os.chdir(\"/home/asaeed9/work/data/2cat/train\")\n",
    "    g = glob('dog*.jpg')\n",
    "    shuf = np.random.permutation(g)\n",
    "    for i in range(ndogs): os.rename(shuf[i], \"../\" + data_src + \"/\" +segment+ \"/dogs/\" + shuf[i])\n",
    "\n",
    "    g = glob('cat*.jpg')\n",
    "    shuf = np.random.permutation(g)\n",
    "    for i in range(ncats): os.rename(shuf[i], \"../\" + data_src + \"/\" +segment+ \"/cats/\" + shuf[i])\n",
    "        \n",
    "#     move_files(path + \"unlabel\", path + \"unlabel/cats\", \"cat*.jpg\")\n",
    "#     move_files(path + \"unlabel\", path + \"unlabel/dogs\", \"dog*.jpg\")\n",
    "\n",
    "    # %mv ../sample/unlabel/cat*.jpg ../sample/unlabel/cats/\n",
    "    # %mv ../sample/unlabel/dog*.jpg ../sample/unlabel/dogs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#move_data(4000, \"unlabel\", False)\n",
    "move_data(4000, \"test\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment Setup\n",
    "* Number of epochs: 5\n",
    "* Batch Size: 64\n",
    "* Initial Training Set: 100\n",
    "* Retrain Image Set Size: 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nepoch = 5\n",
    "batch_size = 64\n",
    "train_size = 100\n",
    "running_train_size = 100\n",
    "tr_model = None\n",
    "i=0\n",
    "retrain_size = 15\n",
    "training_set_size = []\n",
    "valid_size = int(math.floor(.2 * train_size))\n",
    "#print('sample size: {}'.format(train_size + valid_size))\n",
    "loss = 0.0\n",
    "loss_array = []\n",
    "accuracy = 0.0 \n",
    "accuracy_array = []\n",
    "\n",
    "#clean previous data\n",
    "adjust_prev_data(\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size:100\n",
      "Valid Size:20\n",
      "Moved Train: 100 \n",
      "Moved Valid: 20\n",
      "Found 100 images belonging to 2 classes.\n",
      "Found 20 images belonging to 2 classes.\n",
      "Epoch 1/2\n",
      "100/100 [==============================] - 2s - loss: 1.2308 - acc: 0.4700 - val_loss: 2.0686 - val_acc: 0.6000\n",
      "Epoch 2/2\n",
      "100/100 [==============================] - 1s - loss: 0.9050 - acc: 0.5500 - val_loss: 2.4126 - val_acc: 0.6000\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.7534 - acc: 0.5900 - val_loss: 2.1197 - val_acc: 0.6000\n",
      "Epoch 1/2\n",
      "100/100 [==============================] - 2s - loss: 0.7384 - acc: 0.6400 - val_loss: 2.1625 - val_acc: 0.6000\n",
      "Epoch 2/2\n",
      "100/100 [==============================] - 1s - loss: 0.6519 - acc: 0.6900 - val_loss: 1.8214 - val_acc: 0.6000\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"/home/asaeed9/work/data/2cat/train\")\n",
    "print(\"Train Size:{}\".format(train_size))\n",
    "print(\"Valid Size:{}\".format(valid_size))\n",
    "\n",
    "#move images to training directory\n",
    "\n",
    "dog_train_count, dog_valid_count, cat_train_count, cat_valid_count = copy_samples(train_size, valid_size)\n",
    "\n",
    "print(\"Moved Train: {} \\nMoved Valid: {}\".format(dog_train_count + cat_train_count, dog_valid_count + cat_valid_count))    \n",
    "\n",
    "tr_model,file_timestamp = fit(tr_model, path, results_path, nepoch, batch_size, train_size, valid_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size:100\n",
      "Valid Size:20\n",
      "Moved Train: 100 \n",
      "Moved Valid: 20\n",
      "Found 100 images belonging to 2 classes.\n",
      "Found 20 images belonging to 2 classes.\n",
      "Epoch 1/2\n",
      "100/100 [==============================] - 1s - loss: 1.0694 - acc: 0.5100 - val_loss: 1.0175 - val_acc: 0.4000\n",
      "Epoch 2/2\n",
      "100/100 [==============================] - 1s - loss: 1.0209 - acc: 0.5100 - val_loss: 0.8716 - val_acc: 0.4000\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s - loss: 0.9502 - acc: 0.5400 - val_loss: 0.9252 - val_acc: 0.5000\n",
      "Epoch 1/2\n",
      "100/100 [==============================] - 1s - loss: 0.9813 - acc: 0.5900 - val_loss: 0.6418 - val_acc: 0.7000\n",
      "Epoch 2/2\n",
      "100/100 [==============================] - 1s - loss: 0.7398 - acc: 0.6000 - val_loss: 0.7361 - val_acc: 0.6000\n",
      "\n",
      "Verification on Unlabel set.\n",
      "Found 95 images belonging to 2 classes.\n",
      "95/95 [==============================] - 0s     \n",
      "\n",
      "Unlabel Accuracy:0.431578947055\n",
      "Unlabel Loss:1.28010287285\n",
      "Total Dogs: 8, Total Cats: 7\n",
      "Valid Dog: 1, Valid Cat: 1\n",
      "\n",
      "Verification on Test set.\n",
      "Found 100 images belonging to 2 classes.\n",
      " 96/100 [===========================>..] - ETA: 0s\n",
      "Training Set Size:[100]\n",
      "Accuracy:[0.46999999999999997]\n",
      "Loss:[1.0613416862487792]\n",
      "Train Size:115\n",
      "Valid Size:22\n",
      "Moved Train: 100 \n",
      "Moved Valid: 20\n",
      "Found 115 images belonging to 2 classes.\n",
      "Found 22 images belonging to 2 classes.\n",
      "Epoch 1/2\n",
      "115/115 [==============================] - 2s - loss: 0.7305 - acc: 0.6522 - val_loss: 0.9875 - val_acc: 0.5909\n",
      "Epoch 2/2\n",
      "115/115 [==============================] - 1s - loss: 0.8815 - acc: 0.6261 - val_loss: 0.8765 - val_acc: 0.5909\n",
      "Epoch 1/1\n",
      "115/115 [==============================] - 2s - loss: 0.7069 - acc: 0.6522 - val_loss: 0.7520 - val_acc: 0.5909\n",
      "Epoch 1/2\n",
      "115/115 [==============================] - 2s - loss: 0.7600 - acc: 0.6348 - val_loss: 0.7252 - val_acc: 0.6364\n",
      "Epoch 2/2\n",
      "115/115 [==============================] - 1s - loss: 0.8437 - acc: 0.6261 - val_loss: 0.6395 - val_acc: 0.5909\n",
      "\n",
      "Verification on Unlabel set.\n",
      "Found 95 images belonging to 2 classes.\n",
      "95/95 [==============================] - 0s     \n",
      "\n",
      "Unlabel Accuracy:0.421052631579\n",
      "Unlabel Loss:1.06861093546\n",
      "Total Dogs: 9, Total Cats: 6\n",
      "Valid Dog: 1, Valid Cat: 1\n",
      "\n",
      "Verification on Test set.\n",
      "Found 100 images belonging to 2 classes.\n",
      " 96/100 [===========================>..] - ETA: 0s\n",
      "Training Set Size:[100, 115]\n",
      "Accuracy:[0.46999999999999997, 0.46999999999999997]\n",
      "Loss:[1.0613416862487792, 0.89287169933319088]\n",
      "Train Size:130\n",
      "Valid Size:24\n",
      "Moved Train: 100 \n",
      "Moved Valid: 20\n",
      "Found 130 images belonging to 2 classes.\n",
      "Found 24 images belonging to 2 classes.\n",
      "Epoch 1/2\n",
      "130/130 [==============================] - 2s - loss: 0.8774 - acc: 0.6000 - val_loss: 0.6186 - val_acc: 0.6667\n",
      "Epoch 2/2\n",
      "130/130 [==============================] - 1s - loss: 0.8402 - acc: 0.6308 - val_loss: 0.7094 - val_acc: 0.5833\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 2s - loss: 0.8671 - acc: 0.5846 - val_loss: 0.7181 - val_acc: 0.5417\n",
      "Epoch 1/2\n",
      "130/130 [==============================] - 2s - loss: 0.8136 - acc: 0.6000 - val_loss: 0.6885 - val_acc: 0.5833\n",
      "Epoch 2/2\n",
      "130/130 [==============================] - 1s - loss: 0.5265 - acc: 0.8077 - val_loss: 0.7654 - val_acc: 0.5833\n",
      "\n",
      "Verification on Unlabel set.\n",
      "Found 95 images belonging to 2 classes.\n",
      "95/95 [==============================] - 0s     \n",
      "\n",
      "Unlabel Accuracy:0.431578947368\n",
      "Unlabel Loss:1.22883527404\n",
      "Total Dogs: 9, Total Cats: 6\n",
      "Valid Dog: 1, Valid Cat: 1\n",
      "\n",
      "Verification on Test set.\n",
      "Found 100 images belonging to 2 classes.\n",
      " 96/100 [===========================>..] - ETA: 0s\n",
      "Training Set Size:[100, 115, 130]\n",
      "Accuracy:[0.46999999999999997, 0.46999999999999997, 0.46000000000000002]\n",
      "Loss:[1.0613416862487792, 0.89287169933319088, 0.99208953380584719]\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"/home/asaeed9/work/data/2cat/train\")\n",
    "nepoch = 5\n",
    "batch_size = 64\n",
    "train_size = 100\n",
    "tr_model = None\n",
    "i=0\n",
    "retrain_size = 15\n",
    "training_set_size = []\n",
    "valid_size = int(math.floor(.2 * train_size))\n",
    "#print('sample size: {}'.format(train_size + valid_size))\n",
    "loss = 0.0\n",
    "loss_array = []\n",
    "accuracy = 0.0 \n",
    "accuracy_array = []\n",
    "\n",
    "#clean previous data\n",
    "adjust_prev_data(\"train\")\n",
    "dog_train_count, dog_valid_count, cat_train_count, cat_valid_count = copy_samples(train_size, valid_size)\n",
    "print(\"Moved Train: {} \\nMoved Valid: {}\".format(dog_train_count + cat_train_count, dog_valid_count + cat_valid_count))    \n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"Train Size:{}\".format(train_size))\n",
    "    print(\"Valid Size:{}\".format(valid_size))\n",
    "\n",
    "    tr_model,file_timestamp = fit(tr_model, path, results_path, nepoch, batch_size, train_size, valid_size)\n",
    "\n",
    "    model = None\n",
    "    model = get_test_model()\n",
    "    #model.load_weights(results_path+'/ft_' + str(train_size) + '.e' + str(nepoch))\n",
    "    #print('{0}/ft_{1}'.format(last_file_timestamp))\n",
    "    #print('Last File Timestamp- before loading:{}'.format(file_timestamp))\n",
    "    model.load_weights(results_path+'/ft_{}'.format(file_timestamp))\n",
    "\n",
    "    print(\"\\nVerification on Unlabel set.\")\n",
    "    probs, test_batches,loss, accuracy = predict(path, model, \"unlabel\")\n",
    "    print('\\nUnlabel Accuracy:{}'.format(accuracy))\n",
    "    print('Unlabel Loss:{}'.format(loss))\n",
    "\n",
    "    #get the top retrain_size most confused images\n",
    "\n",
    "    retrain_idx = np.argsort(abs(0.5 - probs[:, 1]))[:retrain_size]\n",
    "    retrain_set = [test_batches.filenames[i] for i in retrain_idx]\n",
    "\n",
    "    #print('Retrain Set Length:{}'.format(len(retrain_set)))\n",
    "\n",
    "    ndog = sum('dog' in name for name in retrain_set)\n",
    "    ncat =  sum('cat' in name for name in retrain_set)\n",
    "    valid_dog = int(math.floor(.2*ndog))\n",
    "    valid_cat = int(math.floor(.2*ncat))\n",
    "\n",
    "    print(\"Total Dogs: {}, Total Cats: {}\".format(ndog, ncat))\n",
    "    print(\"Valid Dog: {}, Valid Cat: {}\".format(valid_dog, valid_cat))\n",
    "\n",
    "    # os.chdir(path + 'unlabel')\n",
    "    #move existing training data to the store\n",
    "    #adjust_prev_data_sample(\"used_train\")\n",
    "    #train_size, valid_size, copied_images = move_to_train(retrain_set)\n",
    "    copied_images = move_samples_imbalance(retrain_set, \"train\")\n",
    "    refil_unlabel(ndog, ncat, \"unlabel\") #move removed dogs and cats to unlabel segment again\n",
    "    refil_unlabel(valid_dog, valid_cat, \"valid\") #catch up validation set to training, to maintain 80/20 distribution\n",
    "\n",
    "    print(\"\\nVerification on Test set.\")\n",
    "    model_test = None\n",
    "    model_test = get_test_model()\n",
    "    #model.load_weights(results_path+'/ft_' + str(train_size) + '.e' + str(nepoch))\n",
    "    #print('{0}/ft_{1}'.format(last_file_timestamp))\n",
    "    #print('Last File Timestamp- before loading:{}'.format(file_timestamp))\n",
    "    model_test.load_weights(results_path+'/ft_{}'.format(file_timestamp))\n",
    "\n",
    "    probs, test_batches,loss, accuracy = predict(path, model_test, \"test\")\n",
    "    training_set_size.append(train_size)\n",
    "    loss_array.append(loss)\n",
    "    accuracy_array.append(accuracy)\n",
    "\n",
    "    print('\\nTraining Set Size:{}'.format(training_set_size))\n",
    "    print('Accuracy:{}'.format(accuracy_array))\n",
    "    print('Loss:{}'.format(loss_array))\n",
    "    \n",
    "    train_size, valid_size = get_train_valid_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 2] No such file or directory: '../data/2cat/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-64e079aa8deb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mexisting_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/2cat/train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mexisting_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 2] No such file or directory: '../data/2cat/train'"
     ]
    }
   ],
   "source": [
    "existing_model = 0\n",
    "nepoch = 5\n",
    "batch_size = 64\n",
    "if existing_model:\n",
    "    train_size = 8235\n",
    "    running_train_size = 15560\n",
    "    tr_model = get_train_model()\n",
    "    tr_model.load_weights(results_path+'/ft_20171019171848')\n",
    "    i=1\n",
    "else:\n",
    "    train_size = 100\n",
    "    running_train_size = 100\n",
    "    tr_model = None\n",
    "    i=0\n",
    "retrain_size = 15\n",
    "training_set_size = []\n",
    "valid_size = int(math.floor(.2 * train_size))\n",
    "#print('sample size: {}'.format(train_size + valid_size))\n",
    "loss = 0.0\n",
    "loss_array = []\n",
    "accuracy = 0.0 \n",
    "accuracy_array = []\n",
    "        \n",
    "#copy test data\n",
    "#move_data(2000, 'test', True)\n",
    "#move_data(2000, 'unlabel', True)\n",
    "\n",
    "#    for i in range(3):\n",
    "print(\"Train Size:{}\".format(train_size))\n",
    "print(\"Valid Size:{}\".format(valid_size))\n",
    "\n",
    "if train_size == 0 or valid_size == 0: #handle null case\n",
    "    handle_null(50,10)\n",
    "    train_size += 50\n",
    "    valid_size += 10\n",
    "\n",
    "tr_model,file_timestamp = fit(i, tr_model, path, results_path, nepoch, batch_size, train_size, valid_size)\n",
    "\n",
    "model = None\n",
    "model = get_test_model()\n",
    "#model.load_weights(results_path+'/ft_' + str(train_size) + '.e' + str(nepoch))\n",
    "#print('{0}/ft_{1}'.format(last_file_timestamp))\n",
    "#print('Last File Timestamp- before loading:{}'.format(file_timestamp))\n",
    "model.load_weights(results_path+'/ft_{}'.format(file_timestamp))\n",
    "\n",
    "print(\"\\nVerification on Unlabel set.\")\n",
    "probs, test_batches,loss, accuracy = predict(path, model, \"unlabel\")\n",
    "print('\\nUnlabel Accuracy:{}'.format(accuracy))\n",
    "print('Unlabel Loss:{}'.format(loss))\n",
    "#     training_set_size.append(running_train_size)\n",
    "#     loss_array.append(loss)\n",
    "#     accuracy_array.append(accuracy)\n",
    "\n",
    "#get the top 100, most confused images\n",
    "retrain_idx = np.argsort(abs(0.5 - probs[:, 1]))[:retrain_size]\n",
    "#print(len(retrain_idx))\n",
    "\n",
    "retrain_set = [test_batches.filenames[i] for i in retrain_idx]\n",
    "\n",
    "#print('Retrain Set Length:{}'.format(len(retrain_set)))\n",
    "\n",
    "os.chdir(path + 'unlabel')\n",
    "ndog = sum('dog' in name for name in retrain_set)\n",
    "ncat =  sum('cat' in name for name in retrain_set)\n",
    "limit = min(ncat, ndog)\n",
    "\n",
    "#print('Dogs:{}, Cats:{}'.format(ndog, ncat))\n",
    "\n",
    "#move existing training data to the store\n",
    "#adjust_prev_data_sample(\"used_train\")\n",
    "train_size, valid_size, copied_images = move_to_train(retrain_set, limit)\n",
    "refil_unlabel(copied_images)\n",
    "\n",
    "\n",
    "print(\"\\nVerification on Test set.\")\n",
    "model_test = None\n",
    "model_test = get_test_model()\n",
    "#model.load_weights(results_path+'/ft_' + str(train_size) + '.e' + str(nepoch))\n",
    "#print('{0}/ft_{1}'.format(last_file_timestamp))\n",
    "#print('Last File Timestamp- before loading:{}'.format(file_timestamp))\n",
    "model_test.load_weights(results_path+'/ft_{}'.format(file_timestamp))\n",
    "\n",
    "probs, test_batches,loss, accuracy = predict(path, model_test, \"test\")\n",
    "training_set_size.append(running_train_size)\n",
    "loss_array.append(loss)\n",
    "accuracy_array.append(accuracy)\n",
    "\n",
    "running_train_size += train_size\n",
    "\n",
    "print('\\nTraining Set Size:{}'.format(training_set_size))\n",
    "print('Accuracy:{}'.format(accuracy_array))\n",
    "print('Loss:{}'.format(loss_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
